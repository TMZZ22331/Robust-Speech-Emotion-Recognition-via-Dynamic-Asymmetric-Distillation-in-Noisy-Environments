import torch
import torch.nn as nn
import torch.nn.functional as F

# --- Emotion2VecåŸºç¡€æ¨¡å‹ ---
class Emotion2VecEncoder(nn.Module):
    """åŸºäºemotion2vecçš„ç¼–ç å™¨æ¨¡å—"""
    
    def __init__(self, input_dim=768, hidden_dim=256, pretrained_path=None):
        super().__init__()
        
        # é¢„ç½‘ç»œï¼šå°†emotion2vecç‰¹å¾æ˜ å°„åˆ°éšè—ç»´åº¦
        self.pre_net = nn.Linear(in_features=input_dim, out_features=hidden_dim)
        self.activate = nn.ReLU()
        
        print(f"ğŸ—ï¸ Emotion2Vecç¼–ç å™¨: {input_dim} â†’ {hidden_dim}")
    
    def forward(self, x, padding_mask=None):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: (batch_size, seq_len, input_dim) - emotion2vecç‰¹å¾
            padding_mask: (batch_size, seq_len) - å¡«å……æ©ç ï¼ŒTrueè¡¨ç¤ºå¡«å……ä½ç½®
        Returns:
            encoded_features: (batch_size, hidden_dim) - ç¼–ç åçš„ç‰¹å¾
        """
        # é€šè¿‡é¢„ç½‘ç»œ
        x = self.activate(self.pre_net(x))  # (B, T, hidden_dim)
        
        # å¤„ç†padding maskå¹¶è¿›è¡Œå¹³å‡æ± åŒ–
        if padding_mask is not None:
            # å°†paddingä½ç½®ç½®é›¶
            x = x * (1 - padding_mask.unsqueeze(-1).float())
            # è®¡ç®—æœ‰æ•ˆé•¿åº¦çš„å¹³å‡å€¼
            valid_lengths = (1 - padding_mask.float()).sum(dim=1, keepdim=True)  # (B, 1)
            x = x.sum(dim=1) / torch.clamp(valid_lengths, min=1.0)  # (B, hidden_dim)
        else:
            # æ— maskæ—¶ç›´æ¥å¹³å‡æ± åŒ–
            x = x.mean(dim=1)  # (B, hidden_dim)
            
        return x

# --- åˆ†ç±»å™¨æ¨¡å— ---
class EmotionClassifier(nn.Module):
    """æƒ…ç»ªåˆ†ç±»å™¨"""
    
    def __init__(self, input_dim: int, num_classes: int, dropout_rate: float = 0.1):
        super().__init__()
        
        # æ·»åŠ dropoutæé«˜æ³›åŒ–èƒ½åŠ›
        self.dropout = nn.Dropout(dropout_rate)
        self.fc_layer = nn.Linear(in_features=input_dim, out_features=num_classes)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: (batch_size, input_dim) - ç¼–ç å™¨è¾“å‡ºçš„ç‰¹å¾
        Returns:
            logits: (batch_size, num_classes) - åˆ†ç±»logits
        """
        x = self.dropout(x)
        logits = self.fc_layer(x)
        return logits

# --- SSRLä¸»æ¡†æ¶ ---
class SSRLModel(nn.Module):
    """
    å®Œæ•´çš„ Self-Supervised Reflective Learning (SSRL) æ¶æ„ã€‚
    
    æ•´åˆäº†ï¼š
    1. Emotion2Vecç¼–ç å™¨ä½œä¸ºbackbone
    2. KLæ•£åº¦çº¦æŸï¼ˆæ•™å¸ˆ-å­¦ç”Ÿä¸€è‡´æ€§ï¼‰
    3. ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ˆSCLï¼‰- ç±»åˆ«å¯¹é½
    4. æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰- åŸŸå¯¹é½
    5. Top-KåŠ¨æ€ç½®ä¿¡åº¦ç­›é€‰
    """
    def __init__(self, cfg):
        """
        åˆå§‹åŒ–å®Œæ•´çš„SSRLæ¨¡å‹ã€‚
        
        å‚æ•°:
            cfg: é…ç½®å¯¹è±¡ï¼Œéœ€è¦åŒ…å«ï¼š
                - INPUT_DIM: emotion2vecç‰¹å¾ç»´åº¦ (é€šå¸¸æ˜¯768)
                - HIDDEN_DIM: ç¼–ç å™¨éšè—å±‚ç»´åº¦ (é»˜è®¤256)
                - NUM_CLASSES: åˆ†ç±»ç±»åˆ«æ•° (é€šå¸¸æ˜¯4: ang, hap, neu, sad)
                - EMA_MOMENTUM: æ•™å¸ˆç½‘ç»œEMAæ›´æ–°åŠ¨é‡
                - DROPOUT_RATE: åˆ†ç±»å™¨dropoutç‡
                - PRETRAINED_EMOTION2VEC_PATH: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        """
        super().__init__()
        
        # ä»é…ç½®ä¸­è·å–å‚æ•°
        input_dim = getattr(cfg, 'INPUT_DIM', 768)
        hidden_dim = getattr(cfg, 'HIDDEN_DIM', 256) 
        num_classes = getattr(cfg, 'NUM_CLASSES', 4)
        dropout_rate = getattr(cfg, 'DROPOUT_RATE', 0.1)
        pretrained_path = getattr(cfg, 'PRETRAINED_EMOTION2VEC_PATH', None)
        
        # --- 1. åˆ›å»ºå­¦ç”Ÿç½‘ç»œ ---
        self.student_encoder = Emotion2VecEncoder(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            pretrained_path=None  # å…ˆä¸åŠ è½½ï¼Œåé¢ç»Ÿä¸€åŠ è½½
        )
        self.student_classifier = EmotionClassifier(
            input_dim=hidden_dim,
            num_classes=num_classes,
            dropout_rate=dropout_rate
        )
        
        # --- 2. åˆ›å»ºæ•™å¸ˆç½‘ç»œ ---
        self.teacher_encoder = Emotion2VecEncoder(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            pretrained_path=None  # æ•™å¸ˆç½‘ç»œé€šè¿‡EMAæ›´æ–°ï¼Œä¸ç›´æ¥åŠ è½½é¢„è®­ç»ƒæƒé‡
        )
        self.teacher_classifier = EmotionClassifier(
            input_dim=hidden_dim,
            num_classes=num_classes,
            dropout_rate=0.0  # æ•™å¸ˆç½‘ç»œåœ¨æ¨ç†æ—¶ä¸ä½¿ç”¨dropout
        )
        
        # --- 3. åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæƒé‡ï¼ˆç¼–ç å™¨+åˆ†ç±»å™¨ï¼‰ ---
        if pretrained_path:
            self.load_complete_pretrained_weights(pretrained_path)
        
        # --- 4. åˆå§‹åŒ–æ•™å¸ˆç½‘ç»œ ---
        # å°†æ•™å¸ˆç½‘ç»œçš„æƒé‡åˆå§‹åŒ–ä¸ºä¸å­¦ç”Ÿç½‘ç»œå®Œå…¨ç›¸åŒ
        # å¹¶å°†å…¶è®¾ç½®ä¸ºä¸è®¡ç®—æ¢¯åº¦ï¼Œå› ä¸ºæ•™å¸ˆæ˜¯é€šè¿‡EMAæ›´æ–°çš„
        self._init_teacher_network()
            
        # å­˜å‚¨EMAåŠ¨é‡è¶…å‚æ•°
        self.ema_momentum = getattr(cfg, 'EMA_MOMENTUM', 0.99)
        
        print(f"ğŸš€ å®Œæ•´SSRLæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š è¾“å…¥ç»´åº¦: {input_dim}, éšè—ç»´åº¦: {hidden_dim}, ç±»åˆ«æ•°: {num_classes}")
        print(f"âš–ï¸ EMAåŠ¨é‡: {self.ema_momentum}")
        print(f"ğŸ¯ é›†æˆç»„ä»¶: Emotion2Vec + KLæ•£åº¦ + SCLæŸå¤± + MMDæŸå¤±")
        if pretrained_path:
            print(f"ğŸ¯ é¢„è®­ç»ƒè·¯å¾„: {pretrained_path}")

    def load_complete_pretrained_weights(self, pretrained_path):
        """
        åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæƒé‡ï¼ˆç¼–ç å™¨+åˆ†ç±»å™¨ï¼‰
        ä»ä½ è®­ç»ƒå¥½çš„emotion2vecæ¨¡å‹ä¸­åŠ è½½pre_netå’Œpost_netæƒé‡
        """
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½å®Œæ•´é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
            checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=True)
            
            # æƒé‡æ–‡ä»¶ç›´æ¥åŒ…å«å¼ é‡ï¼Œä¸éœ€è¦åµŒå¥—å­—å…¸
            print(f"ğŸ“‚ å‘ç°æƒé‡é”®: {list(checkpoint.keys())}")
            
            # ç»Ÿè®¡åŠ è½½æƒ…å†µ
            encoder_loaded = 0
            classifier_loaded = 0
            
            # === 1. åŠ è½½ç¼–ç å™¨æƒé‡ (pre_net) ===
            encoder_state_dict = {}
            for key, value in checkpoint.items():
                if key.startswith('pre_net'):
                    encoder_state_dict[key] = value
                    encoder_loaded += 1
            
            if encoder_state_dict:
                self.student_encoder.load_state_dict(encoder_state_dict, strict=False)
                print(f"âœ… ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸ: {encoder_loaded} ä¸ªå‚æ•°")
            
            # === 2. åŠ è½½åˆ†ç±»å™¨æƒé‡ (post_net -> fc_layer) ===
            classifier_state_dict = {}
            for key, value in checkpoint.items():
                if key.startswith('post_net'):
                    # å°†post_netæ˜ å°„åˆ°fc_layer
                    new_key = key.replace('post_net', 'fc_layer')
                    classifier_state_dict[new_key] = value
                    classifier_loaded += 1
            
            if classifier_state_dict:
                self.student_classifier.load_state_dict(classifier_state_dict, strict=False)
                print(f"âœ… åˆ†ç±»å™¨æƒé‡åŠ è½½æˆåŠŸ: {classifier_loaded} ä¸ªå‚æ•°")
            
            # === 3. æ€»ç»“ ===
            total_loaded = encoder_loaded + classifier_loaded
            print(f"ğŸ¯ é¢„è®­ç»ƒæƒé‡åŠ è½½å®Œæˆ: æ€»å…± {total_loaded} ä¸ªå‚æ•°")
            print(f"   ğŸ“Š ç¼–ç å™¨(pre_net): {encoder_loaded} ä¸ªå‚æ•°")
            print(f"   ğŸ¯ åˆ†ç±»å™¨(post_net): {classifier_loaded} ä¸ªå‚æ•°")
            
            if total_loaded == 4:  # 2ä¸ªç¼–ç å™¨ + 2ä¸ªåˆ†ç±»å™¨
                print(f"âœ… æƒé‡åŠ è½½å®Œæ•´ï¼æ¨¡å‹å·²ä½¿ç”¨ä½ è®­ç»ƒå¥½çš„åˆ†ç±»å¤´")
            else:
                print(f"âš ï¸ æƒé‡åŠ è½½ä¸å®Œæ•´ï¼Œéƒ¨åˆ†ä½¿ç”¨éšæœºåˆå§‹åŒ–")
            
        except Exception as e:
            print(f"âŒ åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
            print("ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
            import traceback
            traceback.print_exc()

    def _init_teacher_network(self):
        """åˆå§‹åŒ–æ•™å¸ˆç½‘ç»œæƒé‡å¹¶è®¾ç½®ä¸ºä¸éœ€è¦æ¢¯åº¦"""
        # å¤åˆ¶å­¦ç”Ÿç½‘ç»œæƒé‡åˆ°æ•™å¸ˆç½‘ç»œ
        for teacher_param, student_param in zip(self.teacher_encoder.parameters(), self.student_encoder.parameters()):
            teacher_param.data.copy_(student_param.data)
            teacher_param.requires_grad = False
            
        for teacher_param, student_param in zip(self.teacher_classifier.parameters(), self.student_classifier.parameters()):
            teacher_param.data.copy_(student_param.data)
            teacher_param.requires_grad = False

    @torch.no_grad()
    def update_teacher_ema(self):
        """
        ä½¿ç”¨å­¦ç”Ÿç½‘ç»œçš„æƒé‡ï¼Œé€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ¥æ›´æ–°æ•™å¸ˆç½‘ç»œã€‚
        è¿™ä¸ªå‡½æ•°åº”è¯¥åœ¨æ¯æ¬¡å­¦ç”Ÿç½‘ç»œæƒé‡æ›´æ–°ï¼ˆoptimizer.step()ï¼‰ä¹‹åè°ƒç”¨ã€‚
        """
        # æ›´æ–°ç¼–ç å™¨
        for teacher_param, student_param in zip(self.teacher_encoder.parameters(), self.student_encoder.parameters()):
            teacher_param.data = teacher_param.data * self.ema_momentum + student_param.data * (1.0 - self.ema_momentum)
            
        # æ›´æ–°åˆ†ç±»å™¨
        for teacher_param, student_param in zip(self.teacher_classifier.parameters(), self.student_classifier.parameters()):
            teacher_param.data = teacher_param.data * self.ema_momentum + student_param.data * (1.0 - self.ema_momentum)

    def predict(self, x: torch.Tensor, padding_mask=None, use_teacher=False):
        """
        é¢„æµ‹å‡½æ•°ï¼Œç”¨äºæ¨ç†é˜¶æ®µ
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾
            padding_mask: å¡«å……æ©ç   
            use_teacher: æ˜¯å¦ä½¿ç”¨æ•™å¸ˆç½‘ç»œè¿›è¡Œé¢„æµ‹
            
        è¿”å›:
            logits: é¢„æµ‹logits
        """
        self.eval()
        with torch.no_grad():
            if use_teacher:
                embedding = self.teacher_encoder(x, padding_mask)
                logits = self.teacher_classifier(embedding)
            else:
                embedding = self.student_encoder(x, padding_mask)
                logits = self.student_classifier(embedding)
        return logits

    def get_embeddings(self, x: torch.Tensor, padding_mask=None, use_teacher=False):
        """
        è·å–ç¼–ç å™¨è¾“å‡ºçš„ç‰¹å¾åµŒå…¥
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾
            padding_mask: å¡«å……æ©ç 
            use_teacher: æ˜¯å¦ä½¿ç”¨æ•™å¸ˆç½‘ç»œ
            
        è¿”å›:
            embeddings: ç‰¹å¾åµŒå…¥
        """
        self.eval()
        with torch.no_grad():
            if use_teacher:
                embeddings = self.teacher_encoder(x, padding_mask)
            else:
                embeddings = self.student_encoder(x, padding_mask)
        return embeddings