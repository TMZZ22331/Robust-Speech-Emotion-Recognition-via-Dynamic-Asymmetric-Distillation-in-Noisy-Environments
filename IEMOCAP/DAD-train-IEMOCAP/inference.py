#!/usr/bin/env python3
"""
IEMOCAPè·¨åŸŸæ¨ç†è„šæœ¬
åŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼Œåœ¨ä¸åŒå™ªå£°ç¯å¢ƒä¸‹è¿›è¡Œè·¨åŸŸæµ‹è¯•
æ”¯æŒè‡ªå®šä¹‰æƒé‡è·¯å¾„å’Œæµ‹è¯•æ•°æ®è·¯å¾„
"""

# ===========================================
# ğŸ”§ ç”¨æˆ·è‡ªå®šä¹‰é…ç½®åŒºåŸŸ
# ===========================================

# æƒé‡æ–‡ä»¶è·¯å¾„é…ç½®
WEIGHT_CONFIG = {
    "model_path": r"C:\Users\admin\Desktop\111\iemocap_cross_domain_results\20db\fold_4\models\iemocap_cross_domain_best.pth",  # è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡è·¯å¾„
    "description": "SNR=20dB IEMOCAP å™ªå£°ç¯å¢ƒä¸‹è®­ç»ƒçš„æ¨¡å‹"  # æƒé‡æè¿°
}

# æµ‹è¯•æ•°æ®é…ç½®
TEST_DATA_CONFIG = {
    "test_data_path": r"C:\Users\admin\Desktop\DATA\fix_CASIA\processed_features_noisy_20db\train",  # æµ‹è¯•æ•°æ®è·¯å¾„ï¼ˆå»æ‰æ‰©å±•åï¼‰
    "noise_description": "CASIA_20db",  # å™ªå£°æè¿°ï¼ˆç”¨äºç»“æœå‘½åï¼‰
    "fold_id": 3  # ä½¿ç”¨å“ªä¸ªfoldçš„æ•°æ®åˆ’åˆ†
}

# æ¨ç†é…ç½®
INFERENCE_CONFIG = {
    "batch_size": 32,  # æ¨ç†æ‰¹æ¬¡å¤§å°
    "save_results": True,  # æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æœ
    "generate_plots": True,  # æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    "results_base_dir": "cross_domain_inference_results"  # ç»“æœä¿å­˜åŸºç¡€ç›®å½•
}

# ===========================================
# ğŸ“¦ å¯¼å…¥å’ŒåŸºç¡€è®¾ç½®
# ===========================================

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import logging
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    f1_score, balanced_accuracy_score, precision_recall_fscore_support
)
import json
from datetime import datetime
import re
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥å¿…è¦æ¨¡å—
import config as cfg
from model import SSRLModel
from dataload_noisy import get_cv_dataloaders_noisy

# å¯¼å…¥CASIAæ•°æ®åŠ è½½å™¨
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../CASIA/DAD-train-CASIA'))
from dataload_casia_noisy import create_casia_noisy_dataloaders_with_speaker_isolation

class IEMOCAPCrossDomainInference:
    """
    IEMOCAPè·¨åŸŸæ¨ç†å™¨
    åŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼Œåœ¨æ–°çš„å™ªå£°ç¯å¢ƒä¸‹è¿›è¡Œæµ‹è¯•
    """
    
    def __init__(self, weight_config, test_data_config, inference_config):
        """åˆå§‹åŒ–æ¨ç†å™¨"""
        self.weight_config = weight_config
        self.test_data_config = test_data_config
        self.inference_config = inference_config
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() and cfg.USE_CUDA else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æå–æƒé‡ä¿¡æ¯
        self.source_model_info = self._extract_model_info()
        
        # è®¾ç½®ç»“æœç›®å½•
        self.results_dir = self._setup_results_directory()
        
        logger.info("="*60)
        logger.info("ğŸš€ IEMOCAPè·¨åŸŸæ¨ç†é…ç½®")
        logger.info("="*60)
        logger.info(f"ğŸ“ æƒé‡è·¯å¾„: {self.weight_config['model_path']}")
        logger.info(f"ğŸ“„ æƒé‡æè¿°: {self.weight_config['description']}")
        logger.info(f"ğŸ—‚ï¸ æµ‹è¯•æ•°æ®: {self.test_data_config['test_data_path']}")
        logger.info(f"ğŸ”Š å™ªå£°ç¯å¢ƒ: {self.test_data_config['noise_description']}")
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {self.inference_config['batch_size']}")
        logger.info(f"ğŸ¯ æ¨ç†ç½‘ç»œ: Student Network (å­¦ç”Ÿç½‘ç»œ)")
        logger.info(f"ğŸ’¾ ç»“æœç›®å½•: {self.results_dir}")
        logger.info("="*60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._setup_components()
    
    def _extract_model_info(self):
        """ä»æƒé‡è·¯å¾„ä¸­æå–æ¨¡å‹è®­ç»ƒä¿¡æ¯"""
        model_path = self.weight_config['model_path']
        
        # å°è¯•æå–å™ªå£°ç­‰çº§ä¿¡æ¯
        source_noise = "unknown"
        fold_info = "unknown"
        
        # æå–å™ªå£°ç­‰çº§
        noise_patterns = [r'(\d+db)', r'(\d+)db', r'noisy_(\d+)db']
        for pattern in noise_patterns:
            match = re.search(pattern, model_path.lower())
            if match:
                source_noise = f"{match.group(1)}"
                break
        
        # æå–foldä¿¡æ¯
        fold_match = re.search(r'fold_(\d+)', model_path.lower())
        if fold_match:
            fold_info = f"fold_{fold_match.group(1)}"
        
        return {
            'source_noise': source_noise,
            'fold': fold_info,
            'full_path': model_path
        }
    
    def _setup_results_directory(self):
        """è®¾ç½®ç»“æœä¿å­˜ç›®å½•"""
        base_dir = self.inference_config['results_base_dir']
        
        # åˆ›å»ºç›®å½•åç§°
        source_info = f"{self.source_model_info['source_noise']}_{self.source_model_info['fold']}"
        target_info = f"test_on_{self.test_data_config['noise_description']}"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        results_dir = os.path.join(base_dir, f"{source_info}_to_{target_info}_{timestamp}")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        os.makedirs(results_dir, exist_ok=True)
        if self.inference_config['save_results']:
            os.makedirs(os.path.join(results_dir, "reports"), exist_ok=True)
        if self.inference_config['generate_plots']:
            os.makedirs(os.path.join(results_dir, "plots"), exist_ok=True)
        
        return results_dir
    
    def _setup_components(self):
        """è®¾ç½®æ¨ç†ç»„ä»¶"""
        self._setup_model()
        self._load_weights()
        self._setup_dataloader()
    
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        logger.info("ğŸ—ï¸ åˆå§‹åŒ–SSRLæ¨¡å‹...")
        self.model = SSRLModel(cfg=cfg).to(self.device)
        
        if cfg.PRINT_MODEL_INFO:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        
        logger.info("âœ… SSRLæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _load_weights(self):
        """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        model_path = self.weight_config['model_path']
        
        logger.info(f"ğŸ’¾ åŠ è½½æƒé‡: {model_path}")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        try:
            # åŠ è½½checkpoint
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # æå–æ¨¡å‹çŠ¶æ€å­—å…¸
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                epoch_info = checkpoint.get('epoch', 'unknown')
                logger.info(f"ğŸ“ˆ æƒé‡æ¥æº: Epoch {epoch_info + 1}" if isinstance(epoch_info, int) else f"ğŸ“ˆ æƒé‡æ¥æº: {epoch_info}")
                
                # æ˜¾ç¤ºè®­ç»ƒæ—¶çš„æ€§èƒ½ä¿¡æ¯
                if 'clean_results' in checkpoint and 'noisy_results' in checkpoint:
                    clean_acc = checkpoint['clean_results'].get('weighted_accuracy', 'N/A')
                    noisy_acc = checkpoint['noisy_results'].get('weighted_accuracy', 'N/A')
                    logger.info(f"ğŸ¯ è®­ç»ƒæ—¶æ€§èƒ½ - Clean: {clean_acc:.2f}%, Noisy: {noisy_acc:.2f}%")
            else:
                state_dict = checkpoint
                logger.info("ğŸ“ˆ æƒé‡æ¥æº: ç›´æ¥çŠ¶æ€å­—å…¸")
            
            # åŠ è½½æƒé‡åˆ°æ¨¡å‹
            self.model.load_state_dict(state_dict)
            logger.info("âœ… æƒé‡åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _detect_dataset_type(self, data_path):
        """æ£€æµ‹æ•°æ®é›†ç±»å‹"""
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ¥åˆ¤æ–­æ•°æ®é›†ç±»å‹
        if os.path.exists(f"{data_path}.lbl") and os.path.exists(f"{data_path}.spk"):
            return "CASIA"
        elif os.path.exists(f"{data_path}.emo"):
            return "IEMOCAP"
        else:
            # å°è¯•æ ¹æ®è·¯å¾„ååˆ¤æ–­
            path_lower = data_path.lower()
            if 'casia' in path_lower:
                return "CASIA"
            elif 'iemocap' in path_lower:
                return "IEMOCAP"
            else:
                logger.warning("âš ï¸ æ— æ³•ç¡®å®šæ•°æ®é›†ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨IEMOCAPåŠ è½½å™¨")
                return "IEMOCAP"

    def _setup_dataloader(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        logger.info("ğŸ—‚ï¸ è®¾ç½®æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
        
        test_data_path = self.test_data_config['test_data_path']
        batch_size = self.inference_config['batch_size']
        fold_id = self.test_data_config['fold_id']
        
        try:
            # æ£€æµ‹æ•°æ®é›†ç±»å‹
            dataset_type = self._detect_dataset_type(test_data_path)
            logger.info(f"ğŸ” æ£€æµ‹åˆ°æ•°æ®é›†ç±»å‹: {dataset_type}")
            
            if dataset_type == "CASIA":
                # ä½¿ç”¨CASIAæ•°æ®åŠ è½½å™¨
                logger.info("ğŸ”Š ä½¿ç”¨CASIAæ•°æ®åŠ è½½å™¨è¿›è¡Œæ¨ç†")
                
                # CASIAä½¿ç”¨0-3çš„foldï¼Œéœ€è¦è½¬æ¢
                casia_fold = fold_id - 1 if fold_id > 0 else 0
                casia_fold = max(0, min(3, casia_fold))  # ç¡®ä¿åœ¨0-3èŒƒå›´å†…
                
                logger.info(f"   ğŸ“‹ IEMOCAP fold {fold_id} â†’ CASIA fold {casia_fold}")
                
                # åˆ›å»ºCASIAæ•°æ®åŠ è½½å™¨ï¼Œåªä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ¨ç†
                _, _, self.test_loader = create_casia_noisy_dataloaders_with_speaker_isolation(
                    test_data_path, batch_size, fold=casia_fold
                )
                
                # CASIAç±»åˆ«ä¿¡æ¯
                self.class_names = ['angry', 'happy', 'neutral', 'sad']
                self.num_classes = 4
                
            else:
                # ä½¿ç”¨IEMOCAPæ•°æ®åŠ è½½å™¨
                logger.info("ğŸ”Š ä½¿ç”¨IEMOCAPæ•°æ®åŠ è½½å™¨è¿›è¡Œæ¨ç†")
                _, _, self.test_loader = get_cv_dataloaders_noisy(
                    test_data_path, batch_size, fold_id=fold_id
                )
                
                # è·å–ç±»åˆ«ä¿¡æ¯ï¼ˆä»é…ç½®ä¸­è·å–æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
                if hasattr(cfg, 'CLASS_NAMES'):
                    self.class_names = cfg.CLASS_NAMES
                    self.num_classes = len(cfg.CLASS_NAMES)
                elif hasattr(cfg, 'NUM_CLASSES'):
                    self.num_classes = cfg.NUM_CLASSES
                    self.class_names = [f"Class_{i}" for i in range(self.num_classes)]
                else:
                    # é»˜è®¤æƒ…å†µï¼Œå‡è®¾æ˜¯æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡
                    self.num_classes = 4
                    self.class_names = ['angry', 'happy', 'neutral', 'sad']
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°ç±»åˆ«é…ç½®ï¼Œä½¿ç”¨é»˜è®¤4ç±»æƒ…æ„Ÿåˆ†ç±»")
            
            logger.info(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å™¨è®¾ç½®å®Œæˆ")
            logger.info(f"   ğŸ“Š æµ‹è¯•æ‰¹æ¬¡æ•°: {len(self.test_loader)}")
            logger.info(f"   ğŸ·ï¸ ç±»åˆ«æ•°é‡: {self.num_classes}")
            logger.info(f"   ğŸ“ ç±»åˆ«åç§°: {self.class_names}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å™¨è®¾ç½®å¤±è´¥: {e}")
            raise
    

    
    def run_inference(self):
        """è¿è¡Œæ¨ç†"""
        logger.info("ğŸš€ å¼€å§‹è·¨åŸŸæ¨ç†...")
        
        self.model.eval()
        all_predictions = []
        all_labels = []
        all_probabilities = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.test_loader):
                # æå–æ•°æ®
                feats = batch['net_input']['feats'].to(self.device)
                padding_mask = batch['net_input']['padding_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # ä½¿ç”¨å­¦ç”Ÿç½‘ç»œè¿›è¡Œæ¨ç†ï¼ˆæ¨èæ–¹å¼ï¼‰
                outputs = self.model.predict(feats, padding_mask, use_teacher=False)
                
                # è·å–é¢„æµ‹ç»“æœ
                probabilities = F.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)
                
                # æ”¶é›†ç»“æœ
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
                
                # æ˜¾ç¤ºè¿›åº¦
                if (batch_idx + 1) % 10 == 0:
                    logger.info(f"  å¤„ç†è¿›åº¦: {batch_idx + 1}/{len(self.test_loader)} æ‰¹æ¬¡")
        
        logger.info("âœ… æ¨ç†å®Œæˆï¼Œå¼€å§‹è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        results = self._calculate_metrics(all_labels, all_predictions, all_probabilities)
        
        # ä¿å­˜ç»“æœ
        if self.inference_config['save_results']:
            self._save_results(results, all_labels, all_predictions)
        
        # ç”Ÿæˆå¯è§†åŒ–
        if self.inference_config['generate_plots']:
            self._generate_plots(results)
        
        return results
    
    def _calculate_metrics(self, true_labels, predictions, probabilities):
        """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
        
        # åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(true_labels, predictions) * 100
        weighted_accuracy = balanced_accuracy_score(true_labels, predictions) * 100
        f1_weighted = f1_score(true_labels, predictions, average='weighted', zero_division=0) * 100
        f1_macro = f1_score(true_labels, predictions, average='macro', zero_division=0) * 100
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, predictions, labels=range(self.num_classes))
        
        # æ¯ç±»æŒ‡æ ‡
        precision, recall, f1, support = precision_recall_fscore_support(
            true_labels, predictions, average=None, zero_division=0, labels=range(self.num_classes)
        )
        
        # æ¯ç±»å‡†ç¡®ç‡
        per_class_accuracy = []
        for i in range(self.num_classes):
            if cm[i, :].sum() > 0:
                per_class_accuracy.append(cm[i, i] / cm[i, :].sum())
            else:
                per_class_accuracy.append(0.0)
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        probabilities = np.array(probabilities)
        max_probs = np.max(probabilities, axis=1)
        confidence_stats = {
            'mean_confidence': float(np.mean(max_probs)),
            'std_confidence': float(np.std(max_probs)),
            'min_confidence': float(np.min(max_probs)),
            'max_confidence': float(np.max(max_probs))
        }
        
        results = {
            'overview': {
                'accuracy': accuracy,
                'weighted_accuracy': weighted_accuracy,
                'f1_weighted': f1_weighted,
                'f1_macro': f1_macro,
                'total_samples': len(true_labels)
            },
            'per_class': {
                'class_names': self.class_names,
                'precision': precision.tolist(),
                'recall': recall.tolist(),
                'f1_score': f1.tolist(),
                'accuracy': per_class_accuracy,
                'support': support.tolist()
            },
            'confusion_matrix': cm.tolist(),
            'confidence_stats': confidence_stats,
            'test_info': {
                'source_model': self.source_model_info,
                'test_data': self.test_data_config,
                'inference_config': self.inference_config
            }
        }
        
        return results
    
    def _save_results(self, results, true_labels, predictions):
        """ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºå®Œæ•´æŠ¥å‘Š
        full_report = {
            'experiment_info': {
                'timestamp': timestamp,
                'source_model_path': self.weight_config['model_path'],
                'source_model_description': self.weight_config['description'],
                'test_data_path': self.test_data_config['test_data_path'],
                'test_noise_description': self.test_data_config['noise_description'],
                'cross_domain_type': f"{self.source_model_info['source_noise']} â†’ {self.test_data_config['noise_description']}"
            },
            'results': results,
            'detailed_classification_report': classification_report(
                true_labels, predictions, target_names=self.class_names, output_dict=True, zero_division=0
            )
        }
        
        # ä¿å­˜å®Œæ•´æŠ¥å‘Š
        report_path = os.path.join(self.results_dir, "reports", f"cross_domain_inference_report_{timestamp}.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, indent=4, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ç”¨äºå¿«é€ŸæŸ¥çœ‹
        summary_report = {
            'cross_domain_test': f"{self.source_model_info['source_noise']} â†’ {self.test_data_config['noise_description']}",
            'performance': {
                'accuracy': f"{results['overview']['accuracy']:.2f}%",
                'weighted_accuracy': f"{results['overview']['weighted_accuracy']:.2f}%",
                'weighted_f1': f"{results['overview']['f1_weighted']:.2f}%",
                'macro_f1': f"{results['overview']['f1_macro']:.2f}%"
            },
            'confidence': {
                'mean': f"{results['confidence_stats']['mean_confidence']:.4f}",
                'std': f"{results['confidence_stats']['std_confidence']:.4f}"
            }
        }
        
        summary_path = os.path.join(self.results_dir, "reports", f"quick_summary_{timestamp}.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, indent=4, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ å¿«é€Ÿæ‘˜è¦å·²ä¿å­˜: {summary_path}")
    
    def _generate_plots(self, results):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 10))
        cm = np.array(results['confusion_matrix'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=self.class_names, yticklabels=self.class_names)
        
        title = f'Cross-Domain Confusion Matrix\n{self.source_model_info["source_noise"]} â†’ {self.test_data_config["noise_description"]}'
        plt.title(title, fontsize=14, weight='bold')
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        
        # æ·»åŠ æ€§èƒ½ä¿¡æ¯
        acc_text = f'Accuracy: {results["overview"]["accuracy"]:.2f}%\nWeighted Acc: {results["overview"]["weighted_accuracy"]:.2f}%'
        plt.figtext(0.02, 0.02, acc_text, fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        
        plt.tight_layout()
        cm_path = os.path.join(self.results_dir, "plots", f"confusion_matrix_{timestamp}.png")
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_path}")
        
        # 2. æ¯ç±»æ€§èƒ½æŸ±çŠ¶å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        x = np.arange(len(self.class_names))
        
        # Precision
        ax1.bar(x, results['per_class']['precision'], color='skyblue', alpha=0.7)
        ax1.set_title('Precision per Class', fontweight='bold')
        ax1.set_xlabel('Classes')
        ax1.set_ylabel('Precision')
        ax1.set_xticks(x)
        ax1.set_xticklabels(self.class_names, rotation=45, ha='right')
        ax1.grid(axis='y', alpha=0.3)
        
        # Recall
        ax2.bar(x, results['per_class']['recall'], color='lightcoral', alpha=0.7)
        ax2.set_title('Recall per Class', fontweight='bold')
        ax2.set_xlabel('Classes')
        ax2.set_ylabel('Recall')
        ax2.set_xticks(x)
        ax2.set_xticklabels(self.class_names, rotation=45, ha='right')
        ax2.grid(axis='y', alpha=0.3)
        
        # F1-Score
        ax3.bar(x, results['per_class']['f1_score'], color='lightgreen', alpha=0.7)
        ax3.set_title('F1-Score per Class', fontweight='bold')
        ax3.set_xlabel('Classes')
        ax3.set_ylabel('F1-Score')
        ax3.set_xticks(x)
        ax3.set_xticklabels(self.class_names, rotation=45, ha='right')
        ax3.grid(axis='y', alpha=0.3)
        
        # Accuracy
        ax4.bar(x, results['per_class']['accuracy'], color='gold', alpha=0.7)
        ax4.set_title('Accuracy per Class', fontweight='bold')
        ax4.set_xlabel('Classes')
        ax4.set_ylabel('Accuracy')
        ax4.set_xticks(x)
        ax4.set_xticklabels(self.class_names, rotation=45, ha='right')
        ax4.grid(axis='y', alpha=0.3)
        
        plt.suptitle(f'Per-Class Performance\n{self.source_model_info["source_noise"]} â†’ {self.test_data_config["noise_description"]}', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        metrics_path = os.path.join(self.results_dir, "plots", f"per_class_metrics_{timestamp}.png")
        plt.savefig(metrics_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“ˆ æ¯ç±»æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {metrics_path}")
    
    def print_results_summary(self, results):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ è·¨åŸŸæ¨ç†ç»“æœæ‘˜è¦")
        logger.info("="*60)
        
        logger.info(f"ğŸ”„ è·¨åŸŸç±»å‹: {self.source_model_info['source_noise']} â†’ {self.test_data_config['noise_description']}")
        logger.info(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {results['overview']['total_samples']}")
        logger.info(f"ğŸ¯ å‡†ç¡®ç‡: {results['overview']['accuracy']:.2f}%")
        logger.info(f"âš–ï¸ åŠ æƒå‡†ç¡®ç‡: {results['overview']['weighted_accuracy']:.2f}%")
        logger.info(f"ğŸ† åŠ æƒF1åˆ†æ•°: {results['overview']['f1_weighted']:.2f}%")
        logger.info(f"ğŸ“ˆ å®å¹³å‡F1åˆ†æ•°: {results['overview']['f1_macro']:.2f}%")
        
        logger.info(f"\nğŸ” ç½®ä¿¡åº¦ç»Ÿè®¡:")
        logger.info(f"   å¹³å‡ç½®ä¿¡åº¦: {results['confidence_stats']['mean_confidence']:.4f}")
        logger.info(f"   ç½®ä¿¡åº¦æ ‡å‡†å·®: {results['confidence_stats']['std_confidence']:.4f}")
        logger.info(f"   æœ€ä½ç½®ä¿¡åº¦: {results['confidence_stats']['min_confidence']:.4f}")
        logger.info(f"   æœ€é«˜ç½®ä¿¡åº¦: {results['confidence_stats']['max_confidence']:.4f}")
        
        logger.info(f"\nğŸ“‹ æ¯ç±»æ€§èƒ½è¯¦æƒ…:")
        for i, class_name in enumerate(self.class_names):
            logger.info(f"   {class_name}: "
                       f"Prec={results['per_class']['precision'][i]:.3f}, "
                       f"Rec={results['per_class']['recall'][i]:.3f}, "
                       f"F1={results['per_class']['f1_score'][i]:.3f}, "
                       f"Acc={results['per_class']['accuracy'][i]:.3f}, "
                       f"Support={results['per_class']['support'][i]}")
        
        logger.info("="*60)

def main():
    """ä¸»å‡½æ•°"""
    
    logger.info("ğŸš€ å¯åŠ¨IEMOCAPè·¨åŸŸæ¨ç†è„šæœ¬")
    
    # éªŒè¯é…ç½®
    if not os.path.exists(WEIGHT_CONFIG["model_path"]):
        logger.error(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {WEIGHT_CONFIG['model_path']}")
        logger.error("è¯·æ£€æŸ¥WEIGHT_CONFIGä¸­çš„model_pathé…ç½®")
        return
    
    # éªŒè¯æµ‹è¯•æ•°æ®è·¯å¾„
    test_data_path = TEST_DATA_CONFIG["test_data_path"]
    data_exists = False
    
    # æ£€æŸ¥CASIAæ ¼å¼æ•°æ®æ–‡ä»¶
    if os.path.exists(f"{test_data_path}.npy") and os.path.exists(f"{test_data_path}.lbl"):
        data_exists = True
        logger.info(f"âœ… æ£€æµ‹åˆ°CASIAæ ¼å¼æ•°æ®æ–‡ä»¶")
    # æ£€æŸ¥IEMOCAPæ ¼å¼æ•°æ®æ–‡ä»¶
    elif os.path.exists(f"{test_data_path}.emo"):
        data_exists = True
        logger.info(f"âœ… æ£€æµ‹åˆ°IEMOCAPæ ¼å¼æ•°æ®æ–‡ä»¶")
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    elif os.path.exists(test_data_path):
        data_exists = True
        logger.info(f"âœ… æ£€æµ‹åˆ°æ•°æ®ç›®å½•")
    
    if not data_exists:
        logger.error(f"âŒ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_data_path}")
        logger.error("è¯·æ£€æŸ¥TEST_DATA_CONFIGä¸­çš„test_data_pathé…ç½®")
        logger.error("ç¡®ä¿å­˜åœ¨ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€:")
        logger.error(f"  - CASIAæ ¼å¼: {test_data_path}.npy, {test_data_path}.lbl")
        logger.error(f"  - IEMOCAPæ ¼å¼: {test_data_path}.emo")
        logger.error(f"  - æˆ–è€…ç›®å½•å­˜åœ¨: {test_data_path}")
        return
    
    try:
        # åˆ›å»ºæ¨ç†å™¨
        inferencer = IEMOCAPCrossDomainInference(
            weight_config=WEIGHT_CONFIG,
            test_data_config=TEST_DATA_CONFIG,
            inference_config=INFERENCE_CONFIG
        )
        
        # è¿è¡Œæ¨ç†
        results = inferencer.run_inference()
        
        # æ‰“å°ç»“æœæ‘˜è¦
        inferencer.print_results_summary(results)
        
        logger.info(f"\nâœ… è·¨åŸŸæ¨ç†å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³: {inferencer.results_dir}")
        
    except Exception as e:
        logger.error(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return

if __name__ == "__main__":
    main() 