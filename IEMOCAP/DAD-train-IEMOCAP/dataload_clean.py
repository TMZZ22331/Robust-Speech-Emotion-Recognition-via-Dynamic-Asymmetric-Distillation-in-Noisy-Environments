import os
import logging
import contextlib
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import config as cfg

logger = logging.getLogger(__name__)

# #############################################################################
# ä»¥ä¸‹ä»£ç ä¸æ‚¨æä¾›çš„ç‰ˆæœ¬ç›¸åŒï¼Œæ— éœ€ä¿®æ”¹
# load_emotion2vec_dataset, CleanEmotionDataset, CleanEmotionDatasetFromArrays,
# load_ssl_features
# #############################################################################

def get_session_ids(data_path, num_samples):
    """
    Reads the .emo file to extract session IDs for each utterance.
    """
    session_ids = []
    # The label file is specified by the 'labels' argument in load_emotion2vec_dataset,
    # which is hardcoded to 'emo'. So we look for a .emo file.
    emo_file_path = data_path + ".emo"
    if not os.path.exists(emo_file_path):
        logger.error(f"Emotion file not found at: {emo_file_path}")
        return [None] * num_samples

    with open(emo_file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            # e.g., Ses01_impro01_F000_neu -> 1
            fname = line.split('\t')[0].strip()
            session_id = int(fname[4])
            session_ids.append(session_id)
    
    if len(session_ids) != num_samples:
        logger.warning(f"Number of session IDs ({len(session_ids)}) does not match number of samples ({num_samples}).")

    return session_ids

def get_fold_sessions(fold_id):
    """
    è·å–æŒ‡å®šfoldçš„è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ä¼šè¯
    ä¸é¢„è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´çš„åˆ’åˆ†ç­–ç•¥
    
    Args:
        fold_id (int): foldç¼–å· (1-5)
    
    Returns:
        tuple: (train_sessions, val_session, test_session)
    """
    if fold_id < 1 or fold_id > 5:
        raise ValueError(f"fold_id must be between 1 and 5, got {fold_id}")
    
    # 5æŠ˜äº¤å‰éªŒè¯çš„ä¼šè¯åˆ†é…ï¼ˆä¸é¢„è®­ç»ƒé˜¶æ®µå®Œå…¨ä¸€è‡´ï¼‰
    fold_configs = {
        1: ([1, 2, 3], 4, 5),  # Fold 1: è®­ç»ƒ(1,2,3), éªŒè¯(4), æµ‹è¯•(5)
        2: ([2, 3, 4], 5, 1),  # Fold 2: è®­ç»ƒ(2,3,4), éªŒè¯(5), æµ‹è¯•(1)
        3: ([3, 4, 5], 1, 2),  # Fold 3: è®­ç»ƒ(3,4,5), éªŒè¯(1), æµ‹è¯•(2)
        4: ([4, 5, 1], 2, 3),  # Fold 4: è®­ç»ƒ(4,5,1), éªŒè¯(2), æµ‹è¯•(3)
        5: ([5, 1, 2], 3, 4),  # Fold 5: è®­ç»ƒ(5,1,2), éªŒè¯(3), æµ‹è¯•(4)
    }
    
    return fold_configs[fold_id]

def load_emotion2vec_dataset(data_path, labels='emo', min_length=3, max_length=None):
    """
    åŠ è½½emotion2vecæ ¼å¼çš„æ•°æ®é›†
    """
    sizes = []
    offsets = []
    emo_labels = []
    npy_data = np.load(data_path + ".npy")
    offset = 0
    skipped = 0
    label_file_path = data_path + f".{labels}"
    if not os.path.exists(label_file_path):
        labels = None
        logger.warning(f"æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {label_file_path}")
    with open(data_path + ".lengths", "r") as len_f, open(
        label_file_path, "r"
    ) if labels is not None else contextlib.ExitStack() as lbl_f:
        for line in len_f:
            length = int(line.rstrip())
            lbl = None if labels is None else next(lbl_f).rstrip().split()[1]
            if length >= min_length and (max_length is None or length <= max_length):
                sizes.append(length)
                offsets.append(offset)
                if lbl is not None:
                    emo_labels.append(lbl)
            else:
                skipped += 1
            offset += length
    sizes = np.asarray(sizes)
    offsets = np.asarray(offsets)
    logger.info(f"åŠ è½½äº† {len(offsets)} ä¸ªæ ·æœ¬ï¼Œè·³è¿‡äº† {skipped} ä¸ªæ ·æœ¬")
    return npy_data, sizes, offsets, emo_labels

class CleanEmotionDataset(Dataset):
    def __init__(self, data_path, transform=None, min_length=3, max_length=None):
        self.transform = transform
        self.feats, self.sizes, self.offsets, self.labels = load_emotion2vec_dataset(
            data_path, labels='emo', min_length=min_length, max_length=max_length
        )
        if self.labels:
            self.numerical_labels = [cfg.LABEL_DICT[label] for label in self.labels]
        else:
            self.numerical_labels = None
        self.class_names = list(cfg.LABEL_DICT.keys())
        self.num_classes = len(self.class_names)
        print(f"âœ… å¹²å‡€æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"   ğŸ“Š æ ·æœ¬æ•°é‡: {len(self.sizes)}")
        print(f"   ğŸ“ ç‰¹å¾ç»´åº¦: {self.feats.shape[1]}")
        print(f"   ğŸ·ï¸ ç±»åˆ«æ•°é‡: {self.num_classes}")
        print(f"   ğŸ“‹ ç±»åˆ«æ ‡ç­¾: {self.class_names}")
        if self.labels:
            from collections import Counter
            label_counts = Counter(self.labels)
            for label, count in label_counts.items():
                print(f"      {label}: {count} æ ·æœ¬")
    def __len__(self):
        return len(self.sizes)
    def __getitem__(self, index):
        offset = self.offsets[index]
        size = self.sizes[index]
        end = offset + size
        feats = torch.from_numpy(self.feats[offset:end, :].copy()).float()
        res = {"id": index, "feats": feats}
        if self.numerical_labels is not None:
            res["target"] = self.numerical_labels[index]
        else:
            res["target"] = None
        return res
    def collator(self, samples):
        if len(samples) == 0: return {}
        feats = [s["feats"] for s in samples]
        sizes = [s.shape[0] for s in feats]
        labels = None
        if samples[0]["target"] is not None:
            labels = torch.tensor([s["target"] for s in samples], dtype=torch.long)
        target_size = max(sizes)
        collated_feats = feats[0].new_zeros(len(feats), target_size, feats[0].size(-1))
        padding_mask = torch.BoolTensor(len(feats), target_size).fill_(False)
        for i, (feat, size) in enumerate(zip(feats, sizes)):
            collated_feats[i, :size] = feat
            padding_mask[i, size:] = True
        res = {
            "id": torch.LongTensor([s["id"] for s in samples]),
            "net_input": {"feats": collated_feats, "padding_mask": padding_mask},
            "labels": labels
        }
        return res
    def get_class_names(self): return self.class_names
    def get_num_classes(self): return self.num_classes

class CleanEmotionDatasetFromArrays(Dataset):
    def __init__(self, feats, sizes, offsets, labels):
        self.feats = feats
        self.sizes = sizes
        self.offsets = offsets
        self.labels = labels
        self.class_names = list(cfg.LABEL_DICT.keys())
        self.num_classes = len(self.class_names)
    def __len__(self):
        return len(self.sizes)
    def __getitem__(self, index):
        offset = self.offsets[index]
        size = self.sizes[index]
        end = offset + size
        feats = torch.from_numpy(self.feats[offset:end, :].copy()).float()
        res = {"id": index, "feats": feats, "target": self.labels[index]}
        return res
    def collator(self, samples):
        if len(samples) == 0: return {}
        feats = [s["feats"] for s in samples]
        sizes = [s.shape[0] for s in feats]
        labels = torch.tensor([s["target"] for s in samples], dtype=torch.long)
        target_size = max(sizes)
        collated_feats = feats[0].new_zeros(len(feats), target_size, feats[0].size(-1))
        padding_mask = torch.BoolTensor(len(feats), target_size).fill_(False)
        for i, (feat, size) in enumerate(zip(feats, sizes)):
            collated_feats[i, :size] = feat
            padding_mask[i, size:] = True
        res = {
            "id": torch.LongTensor([s["id"] for s in samples]),
            "net_input": {"feats": collated_feats, "padding_mask": padding_mask},
            "labels": labels
        }
        return res

def load_ssl_features(feature_path, label_dict, max_speech_seq_len=None):
    # ä¿®æ­£ï¼šåœ¨è¿™é‡Œç»Ÿä¸€æ‹¼æ¥/trainè·¯å¾„
    data_path = os.path.join(feature_path, "train")

    data, sizes, offsets, labels = load_emotion2vec_dataset(
        data_path, labels='emo', min_length=1, max_length=max_speech_seq_len
    )
    if labels:
        numerical_labels = [label_dict[elem] for elem in labels]
    else:
        numerical_labels = []
    
    # ä¿®æ­£ï¼šget_session_idsä¹Ÿéœ€è¦åŸºäºæ‹¼æ¥åçš„è·¯å¾„
    num = len(numerical_labels) if numerical_labels else len(sizes)
    session_ids = get_session_ids(data_path, num)

    iemocap_data = {
        "feats": data, "sizes": sizes, "offsets": offsets,
        "labels": numerical_labels, "num": num,
        "session_ids": np.array(session_ids)
    }
    return iemocap_data

# #############################################################################
# é‡ç‚¹ä¿®æ”¹ï¼šä»¥ä¸‹æ˜¯æ–°çš„ã€å®ç°ä¸¥æ ¼è¯´è¯äººéš”ç¦»çš„æ•°æ®åŠ è½½é€»è¾‘
# #############################################################################

def get_cv_dataloaders(data_path, batch_size=None, fold_id=1):
    """
    Creates DataLoaders for a specific fold of 5-fold cross-validation.
    æ¯ä¸ªfoldåˆ’åˆ†ä¸ºï¼š3ä¸ªè®­ç»ƒä¼šè¯ã€1ä¸ªéªŒè¯ä¼šè¯ã€1ä¸ªæµ‹è¯•ä¼šè¯
    ä¸é¢„è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´çš„æ•°æ®åˆ’åˆ†ç­–ç•¥
    
    Args:
        data_path (str): æ•°æ®è·¯å¾„
        batch_size (int): æ‰¹æ¬¡å¤§å°
        fold_id (int): foldç¼–å· (1-5)
    
    Returns:
        tuple: (train_loader, val_loader, test_loader, class_names, num_classes)
    """
    if batch_size is None:
        batch_size = cfg.BATCH_SIZE
    
    print(f"ğŸ“‚ [Clean Data] Loading data for Fold {fold_id}...")
    
    # æ³¨æ„ï¼šè¿™é‡Œçš„data_pathç°åœ¨æ˜¯æ ¹ç›®å½•ï¼Œload_ssl_featuresä¼šå¤„ç†/train
    dataset = load_ssl_features(data_path, cfg.LABEL_DICT)
    
    feats = dataset['feats']
    sizes = dataset['sizes']
    offsets = dataset['offsets']
    labels = np.array(dataset['labels'])
    session_ids = dataset['session_ids']

    # è·å–å½“å‰foldçš„ä¼šè¯åˆ†é…
    train_sessions, val_session, test_session = get_fold_sessions(fold_id)
    
    print(f"ğŸ“Š Fold {fold_id} Session Assignment:")
    print(f"   ğŸ‹ï¸  Training Sessions: {train_sessions}")
    print(f"   ğŸ§ Validation Session: {val_session}")
    print(f"   ğŸ§ª Test Session: {test_session}")

    # æ ¹æ®ä¼šè¯IDåˆ’åˆ†æ•°æ®é›†
    train_indices = np.where(np.isin(session_ids, train_sessions))[0]
    val_indices = np.where(session_ids == val_session)[0]
    test_indices = np.where(session_ids == test_session)[0]

    def create_subset(indices, subset_name):
        """Helper to create a dataset subset from indices."""
        sub_labels = labels[indices].tolist()
        
        # This part is tricky. We need to create a contiguous feature array for the subset.
        sub_feats_list = [feats[offsets[i] : offsets[i] + sizes[i]] for i in indices]
        sub_feats = np.concatenate(sub_feats_list, axis=0)
        sub_sizes = sizes[indices]
        sub_offsets = np.concatenate([np.array([0]), np.cumsum(sub_sizes)[:-1]], dtype=np.int64)

        print(f"   ğŸ“ {subset_name}: {len(indices)} samples")
        
        return CleanEmotionDatasetFromArrays(sub_feats, sub_sizes, sub_offsets, sub_labels)

    train_dataset = create_subset(train_indices, "Training")
    val_dataset = create_subset(val_indices, "Validation")
    test_dataset = create_subset(test_indices, "Test")
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_dataset.collator, num_workers=0, pin_memory=False, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=val_dataset.collator, num_workers=0, pin_memory=False, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collator, num_workers=0, pin_memory=False, shuffle=False)
    
    num_classes = len(cfg.LABEL_DICT)
    class_names = list(cfg.LABEL_DICT.keys())

    print(f"âœ… Clean data loaders created for Fold {fold_id}:")
    print(f"   ğŸ‹ï¸  Training samples: {len(train_dataset)}")
    print(f"   ğŸ§ Validation samples: {len(val_dataset)}")
    print(f"   ğŸ§ª Test samples: {len(test_dataset)}")

    return train_loader, val_loader, test_loader, class_names, num_classes