#!/usr/bin/env python3
"""
ç¡®è®¤åå·®åˆ†æè„šæœ¬
é€šè¿‡è¿½è¸ªå›ºå®šæ ·æœ¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼ªæ ‡ç­¾å˜åŒ–æ¥é‡åŒ–ç¡®è®¤åå·®
åˆ†æDACPé˜²ç«å¢™æœºåˆ¶ä¸æ ‡ç­¾ç¨³å®šæ€§çš„å…³ç³»
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
import os
from collections import defaultdict, Counter
from scipy import stats

def load_analysis_data(log_filepath, history_filepath):
    """åŠ è½½ç¡®è®¤åå·®æ—¥å¿—å’Œè®­ç»ƒå†å²æ•°æ®"""
    print("ğŸ“‚ åŠ è½½åˆ†ææ•°æ®...")
    
    # åŠ è½½ç¡®è®¤åå·®æ—¥å¿—
    if not os.path.exists(log_filepath):
        raise FileNotFoundError(f"ç¡®è®¤åå·®æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_filepath}")
    
    with open(log_filepath, 'r', encoding='utf-8') as f:
        log_data = json.load(f)
    
    df_log = pd.DataFrame(log_data)
    print(f"âœ… ç¡®è®¤åå·®æ—¥å¿—åŠ è½½å®Œæˆï¼Œå…± {len(df_log)} æ¡è®°å½•")
    
    # åŠ è½½è®­ç»ƒå†å²ï¼ˆå¯é€‰ï¼‰
    history = None
    if history_filepath and os.path.exists(history_filepath):
        with open(history_filepath, 'r', encoding='utf-8') as f:
            history = json.load(f)
        print("âœ… è®­ç»ƒå†å²æ•°æ®åŠ è½½å®Œæˆ")
    else:
        print("âš ï¸ è®­ç»ƒå†å²æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡ç›¸å…³åˆ†æ")
    
    return df_log, history

def analyze_label_consistency(df_log, class_names=None):
    """åˆ†æä¼ªæ ‡ç­¾çš„ä¸€è‡´æ€§å’Œç¿»è½¬æƒ…å†µ"""
    print("ğŸ” åˆ†æä¼ªæ ‡ç­¾ä¸€è‡´æ€§...")
    
    if class_names is None:
        class_names = ['angry', 'happy', 'neutral', 'sad']
    
    # åˆ›å»ºæ ·æœ¬-epoché€è§†è¡¨
    df_pivot = df_log.pivot_table(
        index='sample_id', 
        columns='epoch', 
        values='pseudo_label', 
        aggfunc='first'
    )
    
    # è®¡ç®—ç¿»è½¬ç»Ÿè®¡
    total_samples = len(df_pivot)
    total_epochs = len(df_pivot.columns)
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç¿»è½¬æ¬¡æ•°
    flips_per_sample = df_pivot.diff(axis=1).ne(0).sum(axis=1)
    
    # è®¡ç®—ç¿»è½¬ç‡
    flip_rates = flips_per_sample / (total_epochs - 1)
    
    # ç»Ÿè®¡ä¿¡æ¯
    consistency_stats = {
        'total_samples_tracked': total_samples,
        'total_epochs': total_epochs,
        'mean_flips_per_sample': flips_per_sample.mean(),
        'std_flips_per_sample': flips_per_sample.std(),
        'mean_flip_rate': flip_rates.mean(),
        'samples_never_flipped': (flips_per_sample == 0).sum(),
        'samples_highly_unstable': (flips_per_sample > total_epochs * 0.5).sum()
    }
    
    print(f"ğŸ“Š æ ‡ç­¾ä¸€è‡´æ€§ç»Ÿè®¡:")
    print(f"   - è¿½è¸ªæ ·æœ¬æ•°: {consistency_stats['total_samples_tracked']}")
    print(f"   - å¹³å‡ç¿»è½¬æ¬¡æ•°: {consistency_stats['mean_flips_per_sample']:.2f}")
    print(f"   - å¹³å‡ç¿»è½¬ç‡: {consistency_stats['mean_flip_rate']:.2%}")
    print(f"   - ä»æœªç¿»è½¬æ ·æœ¬: {consistency_stats['samples_never_flipped']} ({consistency_stats['samples_never_flipped']/total_samples:.1%})")
    print(f"   - é«˜åº¦ä¸ç¨³å®šæ ·æœ¬: {consistency_stats['samples_highly_unstable']} ({consistency_stats['samples_highly_unstable']/total_samples:.1%})")
    
    return flips_per_sample, flip_rates, df_pivot, consistency_stats

def create_flip_visualizations(flips_per_sample, df_pivot, output_dir):
    """åˆ›å»ºç¿»è½¬æƒ…å†µçš„å¯è§†åŒ–å›¾è¡¨"""
    print("ğŸ“ˆ åˆ›å»ºç¿»è½¬æƒ…å†µå¯è§†åŒ–...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Pseudo-Label Flip Analysis', fontsize=16, fontweight='bold')
    
    # å­å›¾1: ç¿»è½¬æ¬¡æ•°åˆ†å¸ƒç›´æ–¹å›¾
    axes[0,0].hist(flips_per_sample, bins=np.arange(0, flips_per_sample.max() + 2) - 0.5, 
                   alpha=0.7, color='skyblue', edgecolor='black')
    axes[0,0].set_title('Distribution of Label Flips per Sample')
    axes[0,0].set_xlabel('Number of Flips')
    axes[0,0].set_ylabel('Count of Samples')
    axes[0,0].grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    mean_flips = flips_per_sample.mean()
    axes[0,0].axvline(mean_flips, color='red', linestyle='--', 
                     label=f'Mean: {mean_flips:.2f}')
    axes[0,0].legend()
    
    # å­å›¾2: æ¯è½®epochçš„ç¿»è½¬æ•°é‡
    flips_per_epoch = df_pivot.diff(axis=1).ne(0).sum(axis=0)
    axes[0,1].plot(flips_per_epoch.index, flips_per_epoch.values, 
                   marker='o', color='orange', linewidth=2)
    axes[0,1].set_title('Label Flips per Epoch')
    axes[0,1].set_xlabel('Epoch')
    axes[0,1].set_ylabel('Number of Flips')
    axes[0,1].grid(True, alpha=0.3)
    
    # å­å›¾3: æ ·æœ¬ç¨³å®šæ€§çƒ­å›¾ï¼ˆé€‰æ‹©éƒ¨åˆ†æ ·æœ¬ï¼‰
    sample_subset = df_pivot.iloc[:min(20, len(df_pivot))]  # æ˜¾ç¤ºå‰20ä¸ªæ ·æœ¬
    sns.heatmap(sample_subset, cmap='viridis', cbar_kws={'label': 'Pseudo Label'}, 
                ax=axes[1,0])
    axes[1,0].set_title('Pseudo-Label Evolution (Sample Subset)')
    axes[1,0].set_xlabel('Epoch')
    axes[1,0].set_ylabel('Sample ID')
    
    # å­å›¾4: ç¿»è½¬ç´¯ç§¯åˆ†å¸ƒ
    flip_counts = flips_per_sample.value_counts().sort_index()
    cumulative_pct = flip_counts.cumsum() / len(flips_per_sample) * 100
    axes[1,1].bar(flip_counts.index, cumulative_pct.values, alpha=0.7, color='lightcoral')
    axes[1,1].set_title('Cumulative Distribution of Label Flips')
    axes[1,1].set_xlabel('Number of Flips')
    axes[1,1].set_ylabel('Cumulative Percentage (%)')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout(rect=[0, 0.02, 1, 0.96])
    
    # ä¿å­˜å›¾è¡¨
    flip_plot_path = os.path.join(output_dir, 'ä¼ªæ ‡ç­¾ç¿»è½¬åˆ†æå›¾.png')
    plt.savefig(flip_plot_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ç¿»è½¬åˆ†æå›¾å·²ä¿å­˜è‡³: {flip_plot_path}")
    plt.close()
    
    return flip_plot_path

def analyze_dacp_firewall_relationship(df_log, history, output_dir):
    """åˆ†æDACPé˜²ç«å¢™æœºåˆ¶ä¸æ ‡ç­¾ç¨³å®šæ€§çš„å…³ç³»"""
    if history is None or 'dacp_ema_thresholds' not in history:
        print("âš ï¸ ç¼ºå°‘DACPé˜ˆå€¼æ•°æ®ï¼Œè·³è¿‡é˜²ç«å¢™å…³ç³»åˆ†æ")
        return None
    
    print("ğŸ›¡ï¸ åˆ†æDACPé˜²ç«å¢™ä¸æ ‡ç­¾ç¨³å®šæ€§å…³ç³»...")
    
    # å‡†å¤‡æ•°æ®
    df_thresholds = pd.DataFrame(history['dacp_ema_thresholds'])
    warmup_epochs = 30  # å¯ä»¥ä»é…ç½®è·å–
    
    # è®¡ç®—æ¯ä¸ªepochçš„é˜²ç«å¢™æ¿€æ´»æƒ…å†µ
    firewall_activations = (df_thresholds > 1.0).sum(axis=1)
    firewall_epochs = list(range(warmup_epochs + 1, warmup_epochs + 1 + len(firewall_activations)))
    
    # è®¡ç®—æ¯ä¸ªepochçš„æ ‡ç­¾ç¿»è½¬æƒ…å†µ
    df_pivot = df_log.pivot_table(index='sample_id', columns='epoch', values='pseudo_label')
    flips_per_epoch = df_pivot.diff(axis=1).ne(0).sum(axis=0)
    
    # å¯¹é½æ•°æ®ï¼ˆåªè€ƒè™‘éwarmupæœŸé—´ï¼‰
    common_epochs = sorted(set(firewall_epochs) & set(flips_per_epoch.index))
    
    if len(common_epochs) < 5:
        print("âš ï¸ å…¬å…±epochæ•°æ®ä¸è¶³ï¼Œè·³è¿‡ç›¸å…³æ€§åˆ†æ")
        return None
    
    firewall_aligned = [firewall_activations[e - warmup_epochs - 1] for e in common_epochs]
    flips_aligned = [flips_per_epoch[e] for e in common_epochs]
    
    # è®¡ç®—ç›¸å…³æ€§
    correlation, p_value = stats.pearsonr(firewall_aligned, flips_aligned)
    
    print(f"ğŸ”— é˜²ç«å¢™æ¿€æ´»ä¸æ ‡ç­¾ç¿»è½¬ç›¸å…³æ€§:")
    print(f"   - çš®å°”é€Šç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"   - På€¼: {p_value:.4f}")
    print(f"   - æ˜¾è‘—æ€§: {'æ˜¾è‘—' if p_value < 0.05 else 'ä¸æ˜¾è‘—'}")
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('DACP Firewall vs Label Stability Analysis', fontsize=16, fontweight='bold')
    
    # å­å›¾1: æ—¶é—´åºåˆ—å¯¹æ¯”
    ax1 = axes[0]
    color1 = 'tab:blue'
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Number of Label Flips', color=color1)
    line1 = ax1.plot(common_epochs, flips_aligned, color=color1, marker='o', 
                     label='Label Flips', linewidth=2)
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.grid(True, alpha=0.3)
    
    ax2 = ax1.twinx()
    color2 = 'tab:red'
    ax2.set_ylabel('Classes with Threshold > 1', color=color2)
    line2 = ax2.plot(common_epochs, firewall_aligned, color=color2, marker='s', 
                     linestyle='--', label='Firewall Activations', linewidth=2)
    ax2.tick_params(axis='y', labelcolor=color2)
    
    # æ·»åŠ å›¾ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper left')
    
    ax1.set_title(f'Temporal Relationship (Correlation: {correlation:.3f}, p={p_value:.3f})')
    
    # å­å›¾2: æ•£ç‚¹å›¾æ˜¾ç¤ºç›¸å…³æ€§
    axes[1].scatter(firewall_aligned, flips_aligned, alpha=0.7, s=60, color='purple')
    axes[1].set_xlabel('Number of Classes with Firewall Activated (Ï„ > 1)')
    axes[1].set_ylabel('Number of Label Flips')
    axes[1].set_title('Correlation Analysis')
    axes[1].grid(True, alpha=0.3)
    
    # æ·»åŠ æ‹Ÿåˆçº¿
    if len(firewall_aligned) > 1:
        z = np.polyfit(firewall_aligned, flips_aligned, 1)
        p = np.poly1d(z)
        axes[1].plot(firewall_aligned, p(firewall_aligned), "r--", alpha=0.8, 
                    label=f'Fit: y={z[0]:.2f}x+{z[1]:.2f}')
        axes[1].legend()
    
    plt.tight_layout(rect=[0, 0.02, 1, 0.96])
    
    # ä¿å­˜å›¾è¡¨
    firewall_plot_path = os.path.join(output_dir, 'DACPé˜²ç«å¢™ä¸æ ‡ç­¾ç¨³å®šæ€§å…³ç³»å›¾.png')
    plt.savefig(firewall_plot_path, dpi=300, bbox_inches='tight')
    print(f"âœ… é˜²ç«å¢™å…³ç³»åˆ†æå›¾å·²ä¿å­˜è‡³: {firewall_plot_path}")
    plt.close()
    
    return {
        'correlation': correlation,
        'p_value': p_value,
        'plot_path': firewall_plot_path
    }

def analyze_confirmation_patterns(df_log, output_dir):
    """åˆ†æç¡®è®¤åå·®æ¨¡å¼"""
    print("ğŸ¯ åˆ†æç¡®è®¤åå·®æ¨¡å¼...")
    
    # åˆ†æä¸åŒç¡®å®šæ€§åˆ†æ•°èŒƒå›´çš„æ ·æœ¬ç¨³å®šæ€§
    df_log['certainty_range'] = pd.cut(df_log['certainty_score'], 
                                      bins=[0, 0.6, 0.8, 0.9, 1.0], 
                                      labels=['Low(0-0.6)', 'Med(0.6-0.8)', 
                                             'High(0.8-0.9)', 'VHigh(0.9-1.0)'])
    
    # æŒ‰æ ·æœ¬IDå’Œç¡®å®šæ€§èŒƒå›´åˆ†ç»„åˆ†æ
    pattern_analysis = {}
    
    for certainty_range in df_log['certainty_range'].cat.categories:
        subset = df_log[df_log['certainty_range'] == certainty_range]
        if len(subset) > 0:
            # è®¡ç®—è¯¥ç¡®å®šæ€§èŒƒå›´å†…çš„ç¿»è½¬æƒ…å†µ
            pivot = subset.pivot_table(index='sample_id', columns='epoch', 
                                     values='pseudo_label', aggfunc='first')
            if len(pivot.columns) > 1:
                flips = pivot.diff(axis=1).ne(0).sum(axis=1)
                pattern_analysis[certainty_range] = {
                    'sample_count': len(pivot),
                    'mean_flips': flips.mean(),
                    'flip_rate': flips.mean() / (len(pivot.columns) - 1) if len(pivot.columns) > 1 else 0
                }
    
    # å¯è§†åŒ–ç¡®è®¤åå·®æ¨¡å¼
    if pattern_analysis:
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        fig.suptitle('Confirmation Bias Patterns by Certainty Level', 
                     fontsize=16, fontweight='bold')
        
        ranges = list(pattern_analysis.keys())
        mean_flips = [pattern_analysis[r]['mean_flips'] for r in ranges]
        flip_rates = [pattern_analysis[r]['flip_rate'] for r in ranges]
        
        # å¹³å‡ç¿»è½¬æ¬¡æ•°
        axes[0].bar(ranges, mean_flips, color='lightblue', alpha=0.7)
        axes[0].set_title('Mean Flips by Certainty Level')
        axes[0].set_ylabel('Mean Number of Flips')
        axes[0].tick_params(axis='x', rotation=45)
        axes[0].grid(True, alpha=0.3)
        
        # ç¿»è½¬ç‡
        axes[1].bar(ranges, flip_rates, color='lightcoral', alpha=0.7)
        axes[1].set_title('Flip Rate by Certainty Level')
        axes[1].set_ylabel('Flip Rate')
        axes[1].tick_params(axis='x', rotation=45)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout(rect=[0, 0.02, 1, 0.96])
        
        pattern_plot_path = os.path.join(output_dir, 'ç¡®è®¤åå·®æ¨¡å¼åˆ†æå›¾.png')
        plt.savefig(pattern_plot_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ç¡®è®¤åå·®æ¨¡å¼å›¾å·²ä¿å­˜è‡³: {pattern_plot_path}")
        plt.close()
        
        return pattern_analysis, pattern_plot_path
    
    return None, None

def generate_bias_report(consistency_stats, firewall_analysis, pattern_analysis, output_dir):
    """ç”Ÿæˆç¡®è®¤åå·®åˆ†ææŠ¥å‘Š"""
    print("ğŸ“‹ ç”Ÿæˆç¡®è®¤åå·®åˆ†ææŠ¥å‘Š...")
    
    report_data = {
        'analysis_summary': {
            'analysis_type': 'confirmation_bias_analysis',
            'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            'description': 'é€šè¿‡è¿½è¸ªå›ºå®šæ ·æœ¬ä¼ªæ ‡ç­¾å˜åŒ–åˆ†æç¡®è®¤åå·®'
        },
        'label_consistency': consistency_stats,
        'firewall_relationship': firewall_analysis if firewall_analysis else {},
        'confirmation_patterns': pattern_analysis if pattern_analysis else {}
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(output_dir, 'ç¡®è®¤åå·®åˆ†ææŠ¥å‘Š.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=4, ensure_ascii=False)
    
    print(f"âœ… ç¡®è®¤åå·®åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    return report_path

def analyze_bias(log_filepath, history_filepath, output_dir):
    """
    ä¸»åˆ†æå‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„ç¡®è®¤åå·®åˆ†æ
    """
    print("ğŸ” å¼€å§‹ç¡®è®¤åå·®åˆ†æ...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•å’Œåˆ†æå­ç›®å½•
    analysis_subdir = os.path.join(output_dir, "02_ç¡®è®¤åå·®æ·±åº¦åˆ†æ")
    os.makedirs(analysis_subdir, exist_ok=True)
    output_dir = analysis_subdir  # é‡å®šå‘è¾“å‡ºç›®å½•
    
    try:
        # 1. åŠ è½½æ•°æ®
        df_log, history = load_analysis_data(log_filepath, history_filepath)
        
        # 2. åˆ†ææ ‡ç­¾ä¸€è‡´æ€§
        flips_per_sample, flip_rates, df_pivot, consistency_stats = analyze_label_consistency(df_log)
        
        # 3. åˆ›å»ºç¿»è½¬å¯è§†åŒ–
        flip_plot = create_flip_visualizations(flips_per_sample, df_pivot, output_dir)
        
        # 4. åˆ†æDACPé˜²ç«å¢™å…³ç³»
        firewall_analysis = analyze_dacp_firewall_relationship(df_log, history, output_dir)
        
        # 5. åˆ†æç¡®è®¤åå·®æ¨¡å¼
        pattern_analysis, pattern_plot = analyze_confirmation_patterns(df_log, output_dir)
        
        # 6. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report_path = generate_bias_report(consistency_stats, firewall_analysis, 
                                         pattern_analysis, output_dir)
        
        print("\nğŸ‰ ç¡®è®¤åå·®åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - ç¿»è½¬åˆ†æå›¾: {flip_plot}")
        if firewall_analysis and 'plot_path' in firewall_analysis:
            print(f"   - é˜²ç«å¢™å…³ç³»å›¾: {firewall_analysis['plot_path']}")
        if pattern_plot:
            print(f"   - åå·®æ¨¡å¼å›¾: {pattern_plot}")
        print(f"   - åˆ†ææŠ¥å‘Š: {report_path}")
        
        return {
            'flip_plot': flip_plot,
            'firewall_analysis': firewall_analysis,
            'pattern_plot': pattern_plot,
            'report': report_path
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='åˆ†ææ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¡®è®¤åå·®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    python analyze_confirmation_bias.py --log_path /path/to/confirmation_bias_log.json --history_path /path/to/training_history.json
    python analyze_confirmation_bias.py --log_path /path/to/bias_log.json --history_path /path/to/history.json --output ./bias_analysis
        """
    )
    
    parser.add_argument('--log_path', type=str, required=True,
                       help='ç¡®è®¤åå·®æ—¥å¿—JSONæ–‡ä»¶è·¯å¾„ (confirmation_bias_log.json)')
    parser.add_argument('--history_path', type=str, default=None,
                       help='è®­ç»ƒå†å²JSONæ–‡ä»¶è·¯å¾„ (training_history.json, å¯é€‰)')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤ä¸ºæ—¥å¿—æ–‡ä»¶æ‰€åœ¨ç›®å½•)')
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output is None:
        output_directory = os.path.dirname(args.log_path)
    else:
        output_directory = args.output
    
    # æ‰§è¡Œåˆ†æ
    try:
        results = analyze_bias(
            log_filepath=args.log_path,
            history_filepath=args.history_path,
            output_dir=output_directory
        )
        print(f"\nâœ¨ åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜è‡³: {output_directory}")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main()) 