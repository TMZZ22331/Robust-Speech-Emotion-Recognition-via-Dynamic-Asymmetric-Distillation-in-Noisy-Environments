#!/usr/bin/env python3
"""
CASIAå™ªå£°æ•°æ®åŠ è½½å™¨
æ”¯æŒè¯´è¯äººéš”ç¦»çš„4æŠ˜äº¤å‰éªŒè¯
è®­ç»ƒé›†ä¸ä¼ å…¥æ ‡ç­¾ï¼ˆåŠç›‘ç£å­¦ä¹ ï¼‰ï¼ŒéªŒè¯é›†ä¼ å…¥æ ‡ç­¾
"""

import os
import logging
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

# å¯¼å…¥åŸºç¡€Datasetç±»
from dataload_clean import CleanEmotionDatasetFromArrays

logger = logging.getLogger(__name__)

class NoisyEmotionDatasetFromArrays(Dataset):
    """
    å™ªå£°æ•°æ®é›†ç±»ï¼Œæ”¯æŒæ— æ ‡ç­¾è®­ç»ƒå’Œæœ‰æ ‡ç­¾éªŒè¯
    """
    
    def __init__(self, feats, sizes, offsets, labels=None, class_names=None, has_labels=True):
        """
        Args:
            feats: ç‰¹å¾æ•°ç»„
            sizes: æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦
            offsets: åç§»é‡æ•°ç»„
            labels: æ ‡ç­¾æ•°ç»„ï¼ˆå¯é€‰ï¼Œè®­ç»ƒæ—¶ä¸ºNoneï¼‰
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            has_labels: æ˜¯å¦åŒ…å«æ ‡ç­¾
        """
        self.feats = feats
        self.sizes = sizes
        self.offsets = offsets
        self.labels = labels
        self.class_names = class_names or []
        self.has_labels = has_labels
        
        if has_labels and labels is None:
            raise ValueError("has_labels=Trueä½†æœªæä¾›æ ‡ç­¾")
        
        self.num_samples = len(sizes)
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        offset = self.offsets[idx]
        size = self.sizes[idx]
        
        # æå–ç‰¹å¾
        feat = self.feats[offset:offset + size]
        
        # æ„å»ºæ ·æœ¬å­—å…¸
        sample = {
            'net_input': {
                'feats': torch.FloatTensor(feat),
                'padding_mask': torch.BoolTensor([False] * size)  # æ— å¡«å……
            }
        }
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œæ·»åŠ æ ‡ç­¾
        if self.has_labels and self.labels is not None:
            sample['labels'] = torch.LongTensor([self.labels[idx]])
        
        return sample
    
    def collator(self, samples):
        """æ‰¹æ¬¡æ•´ç†å‡½æ•°"""
        if len(samples) == 0:
            return {}
        
        # æå–ç‰¹å¾å’Œå¡«å……æ©ç 
        feats = [s['net_input']['feats'] for s in samples]
        padding_masks = [s['net_input']['padding_mask'] for s in samples]
        
        # è®¡ç®—æœ€å¤§é•¿åº¦
        max_len = max(feat.size(0) for feat in feats)
        feat_dim = feats[0].size(1)
        
        # åˆ›å»ºæ‰¹æ¬¡å¼ é‡
        batch_feats = torch.zeros(len(samples), max_len, feat_dim)
        batch_padding_masks = torch.ones(len(samples), max_len, dtype=torch.bool)
        
        # å¡«å……ç‰¹å¾å’Œæ©ç 
        for i, (feat, mask) in enumerate(zip(feats, padding_masks)):
            feat_len = feat.size(0)
            batch_feats[i, :feat_len] = feat
            batch_padding_masks[i, :feat_len] = mask
        
        batch = {
            'net_input': {
                'feats': batch_feats,
                'padding_mask': batch_padding_masks
            }
        }
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œæ·»åŠ æ ‡ç­¾
        if self.has_labels and 'labels' in samples[0]:
            labels = [s['labels'] for s in samples]
            batch['labels'] = torch.cat(labels)
        
        return batch

def load_casia_noisy_data(feature_path, label_dict):
    """
    åŠ è½½CASIAå™ªå£°æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç‰¹å¾ã€æ ‡ç­¾å’Œè¯´è¯äººID
    
    Args:
        feature_path: ç‰¹å¾æ–‡ä»¶è·¯å¾„å‰ç¼€ï¼ˆä¸å«æ‰©å±•åï¼‰
        label_dict: æ ‡ç­¾æ˜ å°„å­—å…¸
        
    Returns:
        dict: åŒ…å«ç‰¹å¾ã€æ ‡ç­¾ã€è¯´è¯äººç­‰ä¿¡æ¯çš„æ•°æ®é›†
    """
    logger.info(f"ğŸ“‚ åŠ è½½CASIAå™ªå£°æ•°æ®: {feature_path}")
    
    # åŠ è½½ç‰¹å¾
    feats = np.load(f"{feature_path}.npy")
    logger.info(f"   ç‰¹å¾å½¢çŠ¶: {feats.shape}")
    
    # åŠ è½½æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦
    with open(f"{feature_path}.lengths", "r") as f:
        sizes = [int(line.strip()) for line in f]
    logger.info(f"   æ ·æœ¬æ•°é‡: {len(sizes)}")
    
    # åŠ è½½æ ‡ç­¾å¹¶è½¬æ¢ä¸ºæ•°å­—
    with open(f"{feature_path}.lbl", "r", encoding="utf-8") as f:
        labels = [label_dict[line.strip()] for line in f]
    logger.info(f"   æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
    
    # åŠ è½½è¯´è¯äººID
    with open(f"{feature_path}.spk", "r", encoding="utf-8") as f:
        speakers = [line.strip() for line in f]
    
    # ç»Ÿè®¡è¯´è¯äººä¿¡æ¯
    unique_speakers = np.unique(speakers)
    logger.info(f"   è¯´è¯äººæ•°é‡: {len(unique_speakers)}")
    logger.info(f"   è¯´è¯äººID: {unique_speakers}")
    
    # è®¡ç®—åç§»é‡
    offsets = np.concatenate([[0], np.cumsum(sizes)[:-1]])
    
    dataset = {
        "feats": feats,
        "sizes": np.array(sizes),
        "offsets": np.array(offsets),
        "labels": np.array(labels),
        "speakers": np.array(speakers),
        "num": len(labels),
    }
    
    logger.info(f"âœ… CASIAå™ªå£°æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {dataset['num']} ä¸ªæ ·æœ¬")
    return dataset

def create_casia_noisy_speaker_isolated_loaders(dataset, fold, batch_size, class_names):
    """
    ä¸ºCASIAå™ªå£°æ•°æ®é›†åˆ›å»ºä¸¥æ ¼æŒ‰è¯´è¯äººéš”ç¦»çš„æ•°æ®åŠ è½½å™¨
    4æŠ˜äº¤å‰éªŒè¯å®ç°
    è®­ç»ƒé›†ä¸ä¼ å…¥æ ‡ç­¾ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä¼ å…¥æ ‡ç­¾
    
    Args:
        dataset: æ•°æ®é›†å­—å…¸
        fold: å½“å‰æŠ˜æ•° (0-3)
        batch_size: æ‰¹æ¬¡å¤§å°
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        
    Returns:
        tuple: (student_loader, teacher_loader, val_loader, test_loader)
    """
    all_speakers = np.unique(dataset['speakers'])
    if len(all_speakers) != 4:
        raise ValueError(f"é¢„æœŸCASIAæœ‰4ä¸ªè¯´è¯äººï¼Œä½†æ‰¾åˆ°äº† {len(all_speakers)} ä¸ª")
    
    # ç¡®å®šå½“å‰æŠ˜çš„è¯´è¯äººåˆ†é…
    test_speaker = all_speakers[fold]
    val_speaker = all_speakers[(fold + 1) % 4]  # ä¸‹ä¸€ä¸ªè¯´è¯äººåšéªŒè¯
    train_speakers = [spk for spk in all_speakers if spk not in [test_speaker, val_speaker]]
    
    logger.info("ğŸ“Š CASIAå™ªå£°æ•°æ®è¯´è¯äººéš”ç¦»ç­–ç•¥ (4æŠ˜äº¤å‰éªŒè¯):")
    logger.info(f"   å½“å‰æŠ˜: {fold + 1}/4")
    logger.info(f"   ğŸ‹ï¸  è®­ç»ƒè¯´è¯äºº: {train_speakers}")
    logger.info(f"   ğŸ§ éªŒè¯è¯´è¯äºº: {val_speaker}")
    logger.info(f"   ğŸ§ª æµ‹è¯•è¯´è¯äºº: {test_speaker}")
    
    # æ ¹æ®è¯´è¯äººIDè·å–æ ·æœ¬ç´¢å¼•
    train_indices = np.where(np.isin(dataset['speakers'], train_speakers))[0]
    val_indices = np.where(dataset['speakers'] == val_speaker)[0]
    test_indices = np.where(dataset['speakers'] == test_speaker)[0]
    
    # æ‰“ä¹±è®­ç»ƒé›†
    np.random.shuffle(train_indices)
    
    logger.info(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(train_indices)}")
    logger.info(f"   éªŒè¯æ ·æœ¬æ•°: {len(val_indices)}")
    logger.info(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(test_indices)}")
    
    def create_subset(indices, subset_name, has_labels=True):
        """ä»ç´¢å¼•åˆ—è¡¨åˆ›å»ºæ•°æ®é›†å­é›†"""
        sub_labels = dataset['labels'][indices] if has_labels else None
        sub_sizes = dataset['sizes'][indices]
        sub_offsets = dataset['offsets'][indices]
        
        # ç»Ÿè®¡å­é›†æ ‡ç­¾åˆ†å¸ƒï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
        if has_labels:
            label_counts = np.bincount(sub_labels, minlength=len(class_names))
            logger.info(f"   {subset_name}æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(class_names, label_counts))}")
        else:
            logger.info(f"   {subset_name}æ— æ ‡ç­¾è®­ç»ƒ")
        
        # åˆ›å»ºè¿ç»­çš„ç‰¹å¾æ•°ç»„
        new_feats_list = [dataset['feats'][offset:offset+size] 
                         for offset, size in zip(sub_offsets, sub_sizes)]
        
        new_feats = np.concatenate(new_feats_list, axis=0)
        new_sizes = np.array([feat.shape[0] for feat in new_feats_list])
        new_offsets = np.concatenate([np.array([0]), np.cumsum(new_sizes)[:-1]], dtype=np.int64)
        
        return NoisyEmotionDatasetFromArrays(
            new_feats, new_sizes, new_offsets, sub_labels, 
            class_names, has_labels
        )
    
    # åˆ›å»ºæ•°æ®é›†
    # è®­ç»ƒé›†ï¼šæ— æ ‡ç­¾ï¼ˆåŠç›‘ç£å­¦ä¹ ï¼‰
    train_dataset_student = create_subset(train_indices, "å­¦ç”Ÿè®­ç»ƒé›†", has_labels=False)
    train_dataset_teacher = create_subset(train_indices, "æ•™å¸ˆè®­ç»ƒé›†", has_labels=False)
    
    # éªŒè¯é›†ï¼šæœ‰æ ‡ç­¾
    val_dataset = create_subset(val_indices, "éªŒè¯é›†", has_labels=True)
    
    # æµ‹è¯•é›†ï¼šæœ‰æ ‡ç­¾
    test_dataset = create_subset(test_indices, "æµ‹è¯•é›†", has_labels=True)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    student_loader = DataLoader(
        train_dataset_student, 
        batch_size=batch_size, 
        collate_fn=train_dataset_student.collator,
        num_workers=0, 
        pin_memory=False, 
        shuffle=True
    )
    
    teacher_loader = DataLoader(
        train_dataset_teacher, 
        batch_size=batch_size, 
        collate_fn=train_dataset_teacher.collator,
        num_workers=0, 
        pin_memory=False, 
        shuffle=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        collate_fn=val_dataset.collator,
        num_workers=0, 
        pin_memory=False, 
        shuffle=False
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        collate_fn=test_dataset.collator,
        num_workers=0, 
        pin_memory=False, 
        shuffle=False
    )
    
    logger.info("âœ… CASIAå™ªå£°æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"   ğŸ‹ï¸  è®­ç»ƒæ ·æœ¬ (Student/Teacher): {len(train_dataset_student)}")
    logger.info(f"   ğŸ§ éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    logger.info(f"   ğŸ§ª æµ‹è¯•æ ·æœ¬: {len(test_dataset)}")
    
    return student_loader, teacher_loader, val_loader, test_loader

def create_casia_noisy_dataloaders_with_speaker_isolation(data_path, batch_size, fold=0):
    """
    åˆ›å»ºCASIAå™ªå£°æ•°æ®çš„æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒè¯´è¯äººéš”ç¦»
    
    Args:
        data_path: æ•°æ®è·¯å¾„
        batch_size: æ‰¹æ¬¡å¤§å°
        fold: å½“å‰æŠ˜æ•° (0-3)
        
    Returns:
        tuple: (student_loader, teacher_loader, val_loader, test_loader)
    """
    # CASIAæ ‡ç­¾æ˜ å°„
    label_dict = {
        'angry': 0,
        'happy': 1,
        'neutral': 2,
        'sad': 3
    }
    
    class_names = ['angry', 'happy', 'neutral', 'sad']
    
    logger.info(f"ğŸ“‚ åˆ›å»ºCASIAå™ªå£°æ•°æ®åŠ è½½å™¨ (Fold {fold+1})...")
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_casia_noisy_data(data_path, label_dict)
    
    # åˆ›å»ºè¯´è¯äººéš”ç¦»çš„æ•°æ®åŠ è½½å™¨
    student_loader, teacher_loader, val_loader, test_loader = create_casia_noisy_speaker_isolated_loaders(
        dataset, fold, batch_size, class_names
    )
    
    return student_loader, teacher_loader, val_loader, test_loader

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    import config_casia as cfg
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    student_loader, teacher_loader, val_loader = create_casia_noisy_dataloaders_with_speaker_isolation(
        cfg.NOISY_FEAT_PATH, cfg.BATCH_SIZE, fold=0
    )
    
    print("æµ‹è¯•å®Œæˆ")
    
    # æµ‹è¯•å­¦ç”Ÿè®­ç»ƒæ‰¹æ¬¡ï¼ˆæ— æ ‡ç­¾ï¼‰
    print("å­¦ç”Ÿè®­ç»ƒæ‰¹æ¬¡æµ‹è¯•:")
    for batch in student_loader:
        print(f"  æ‰¹æ¬¡å½¢çŠ¶: {batch['net_input']['feats'].shape}")
        print(f"  æœ‰æ ‡ç­¾: {'labels' in batch}")
        break
    
    # æµ‹è¯•éªŒè¯æ‰¹æ¬¡ï¼ˆæœ‰æ ‡ç­¾ï¼‰
    print("éªŒè¯æ‰¹æ¬¡æµ‹è¯•:")
    for batch in val_loader:
        print(f"  æ‰¹æ¬¡å½¢çŠ¶: {batch['net_input']['feats'].shape}")
        print(f"  æ ‡ç­¾å½¢çŠ¶: {batch['labels'].shape}")
        break 