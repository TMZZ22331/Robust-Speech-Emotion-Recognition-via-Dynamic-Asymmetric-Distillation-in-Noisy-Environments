#!/usr/bin/env python3
"""
CASIAè·¨åŸŸè®­ç»ƒè„šæœ¬ (DACP+ECDA å¢å¼ºç‰ˆ)
ä»é¢„è®­ç»ƒçš„CASIAå¹²å‡€æƒé‡å¼€å§‹ï¼Œåœ¨ä¸åŒå™ªå£°ç¯å¢ƒä¸‹è¿›è¡Œè·¨åŸŸæµ‹è¯•ã€‚
- å®ç° DACP å’Œ ECDA æ–°æ–¹æ³•
- è‡ªåŠ¨è¯†åˆ«å™ªå£°dB
- ç»“æ„åŒ–ä¿å­˜ç»“æœ
- ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import logging
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    f1_score, balanced_accuracy_score, precision_recall_fscore_support
)
import json
from datetime import datetime
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥CASIAé…ç½®å’Œæ¨¡å—
import config_casia as cfg
from model import SSRLModel
from dataload_casia_clean import create_casia_clean_dataloaders
from dataload_casia_noisy import create_casia_noisy_dataloaders_with_speaker_isolation
from utils import (
    DataAugmentation, 
    DACPManager, # å¯¼å…¥æ–°æ¨¡å—
    ECDALoss,    # å¯¼å…¥æ–°æ¨¡å—
)

class FixedCASIACrossDomainTrainer:
    """
    CASIAè·¨åŸŸè®­ç»ƒå™¨ (DACP+ECDA å¢å¼ºç‰ˆ)
    ä»é¢„è®­ç»ƒçš„å¹²å‡€æƒé‡å¼€å§‹ï¼Œåœ¨å™ªå£°ç¯å¢ƒä¸‹è¿›è¡Œè·¨åŸŸé€‚åº”
    """
    
    def __init__(self, fold=3, experiment_name=None):  # é»˜è®¤ä½¿ç”¨ç¬¬4æŠ˜æƒé‡
        """åˆå§‹åŒ–è·¨åŸŸè®­ç»ƒå™¨"""
        self.experiment_name = experiment_name
        log_exp_name = f" ({self.experiment_name})" if self.experiment_name else ""
        logger.info(f"ğŸš€ åˆå§‹åŒ–CASIAè·¨åŸŸè®­ç»ƒå™¨ (DACP+ECDA å¢å¼ºç‰ˆ){log_exp_name} (Fold {fold+1}/4)...")
        
        # åŸºæœ¬é…ç½®
        self.fold = fold
        self.device = torch.device("cuda" if torch.cuda.is_available() and cfg.USE_CUDA else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æå–å™ªå£°ä¿¡æ¯ï¼ˆrootç±»å‹ã€å™ªå£°ç±»å‹ã€å™ªå£°å¼ºåº¦ï¼‰
        self.noise_info = self._extract_noise_info()
        self.results_dir = self._setup_results_directory(self.experiment_name)
        
        # SSRLè®­ç»ƒé…ç½®
        self.WARMUP_EPOCHS = cfg.WARMUP_EPOCHS
        self.scl_start_epoch = cfg.SCL_START_EPOCH
        
        # æ–°çš„ECDAæƒé‡
        self.target_ecda_weight = cfg.WEIGHT_ECDA
        self.weight_ecda = 0.0
        
        # SCL æƒé‡
        self.target_scl_weight = cfg.TARGET_SCL_WEIGHT
        self.weight_scl = 0.0
        
        # æ¸è¿›å¼æƒé‡
        self.initial_consistency_weight = cfg.INITIAL_CONSISTENCY_WEIGHT if cfg.PROGRESSIVE_TRAINING else cfg.WEIGHT_CONSISTENCY
        self.final_consistency_weight = cfg.FINAL_CONSISTENCY_WEIGHT if cfg.PROGRESSIVE_TRAINING else cfg.WEIGHT_CONSISTENCY
        self.current_consistency_weight = self.initial_consistency_weight
        
        # å¢å¼ºçš„æ€§èƒ½è·Ÿè¸ª
        self.best_clean_acc = 0.0
        self.best_noisy_acc = 0.0
        self.best_clean_weighted_f1 = 0.0
        self.best_noisy_weighted_f1 = 0.0
        self.best_clean_weighted_acc = 0.0
        self.best_noisy_weighted_acc = 0.0
        
        self.best_results = {
            'epoch': 0, 'clean_results': None, 'noisy_results': None,
            'clean_confusion_matrix': None, 'noisy_confusion_matrix': None
        }
        
        self.training_history = defaultdict(list)
        self.patience_counter = 0
        
        # [æ–°å¢] åˆå§‹åŒ– DACP ç®¡ç†å™¨å’Œé”šç‚¹
        self.dacp_manager = None
        self.calibrated_anchors = None
        
        logger.info("ğŸ“‹ CASIAè·¨åŸŸè®­ç»ƒé…ç½® (DACP+ECDA å¢å¼ºç‰ˆ):")
        logger.info(f"    - æ•°æ®æº: CASIAå¹²å‡€æ•°æ® â†’ CASIA {self.noise_info['display_name']} å™ªå£°æ•°æ®")
        logger.info(f"    - é¢„è®­ç»ƒæƒé‡: {cfg.PRETRAINED_CASIA_WEIGHT}")
        logger.info(f"    - ç»“æœç›®å½•: {self.results_dir}")
        logger.info(f"    - äº¤å‰éªŒè¯: Fold {fold+1}/4")
        logger.info(f"    - é¢„çƒ­é˜¶æ®µ: {self.WARMUP_EPOCHS} epochs")
        logger.info(f"    - æ ¸å¿ƒç›‘æ§æŒ‡æ ‡: å™ªå£°åŸŸåŠ æƒå‡†ç¡®ç‡")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._setup_components()

    def _extract_noise_info(self):
        """ä»å™ªå£°æ•°æ®è·¯å¾„ä¸­æå–å™ªå£°ä¿¡æ¯ï¼ˆrootç±»å‹ã€å™ªå£°ç±»å‹ã€å™ªå£°å¼ºåº¦ï¼‰"""
        noisy_path = cfg.NOISY_DATA_DIR
        logger.info(f"ğŸ” è§£æå™ªå£°è·¯å¾„: {noisy_path}")
        
        # æå–å®Œæ•´çš„è·¯å¾„åˆ†æ
        import re
        
        # å°è¯•åŒ¹é…å¤šç§æ ¼å¼æ¨¡å¼
        processed_features_pattern = r'processed_features_([^_]+)_(\d+)db'  # processed_features_babble_20db
        root1_pattern_with_wav = r'root1-([^.]+)\.wav-(\d+)db'            # root1-babble.wav-20db
        root1_pattern_without_wav = r'root1-([^-]+)-(\d+)db'              # root1-babble-20db
        root2_pattern = r'root2-(\d+)db'                                  # root2-20db
        
        # é¦–å…ˆå°è¯•æ–°çš„ processed_features æ ¼å¼ï¼ˆé¢„å¤„ç†è„šæœ¬ç”Ÿæˆçš„æ ¼å¼ï¼‰
        processed_match = re.search(processed_features_pattern, noisy_path, re.IGNORECASE)
        if processed_match:
            noise_type = processed_match.group(1)  # babble, factory1, hfchannel, volvo
            db_value = processed_match.group(2)    # 20, 15, 10, 5, 0
            
            noise_info = {
                'root_type': 'real_noise',
                'noise_type': noise_type,
                'db_value': f"{db_value}db",
                'display_name': f"real-{noise_type}-{db_value}db"
            }
            logger.info(f"âœ… æˆåŠŸè¯†åˆ«çœŸå®å™ªå£°: ç±»å‹={noise_type}, å¼ºåº¦={db_value}dB")
            return noise_info
        
        # ç„¶åå°è¯•root1æ ¼å¼ï¼ˆå¸¦.wavï¼‰
        root1_match = re.search(root1_pattern_with_wav, noisy_path, re.IGNORECASE)
        if root1_match:
            noise_type = root1_match.group(1)  # babble
            db_value = root1_match.group(2)    # 20
            
            noise_info = {
                'root_type': 'root1',
                'noise_type': noise_type,
                'db_value': f"{db_value}db",
                'display_name': f"root1-{noise_type}-{db_value}db"
            }
            logger.info(f"âœ… æˆåŠŸè¯†åˆ«Root1å™ªå£°ï¼ˆå¸¦.wavï¼‰: ç±»å‹={noise_type}, å¼ºåº¦={db_value}dB")
            return noise_info
        
        # ç„¶åå°è¯•root1æ ¼å¼ï¼ˆä¸å¸¦.wavï¼‰
        root1_match = re.search(root1_pattern_without_wav, noisy_path, re.IGNORECASE)
        if root1_match:
            noise_type = root1_match.group(1)  # babble
            db_value = root1_match.group(2)    # 20
            
            noise_info = {
                'root_type': 'root1',
                'noise_type': noise_type,
                'db_value': f"{db_value}db",
                'display_name': f"root1-{noise_type}-{db_value}db"
            }
            logger.info(f"âœ… æˆåŠŸè¯†åˆ«Root1å™ªå£°ï¼ˆä¸å¸¦.wavï¼‰: ç±»å‹={noise_type}, å¼ºåº¦={db_value}dB")
            return noise_info
        
        # å°è¯•root2æ ¼å¼
        root2_match = re.search(root2_pattern, noisy_path, re.IGNORECASE)
        if root2_match:
            db_value = root2_match.group(1)    # 20
            
            noise_info = {
                'root_type': 'root2',
                'noise_type': None,  # root2ä¸éœ€è¦å™ªå£°ç±»å‹
                'db_value': f"{db_value}db",
                'display_name': f"root2-{db_value}db"
            }
            logger.info(f"âœ… æˆåŠŸè¯†åˆ«Root2å™ªå£°: å¼ºåº¦={db_value}dB")
            return noise_info
        
        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•é€šç”¨dbæå–
        db_patterns = [r'(\d+)db', r'(-?\d+)_?db']
        for pattern in db_patterns:
            match = re.search(pattern, noisy_path, re.IGNORECASE)
            if match:
                db_value = match.group(1)
                noise_info = {
                    'root_type': 'unknown',
                    'noise_type': 'unknown',
                    'db_value': f"{db_value}db",
                    'display_name': f"unknown-{db_value}db"
                }
                logger.warning(f"âš ï¸ ä½¿ç”¨é€šç”¨æ ¼å¼è¯†åˆ«å™ªå£°: {db_value}dB")
                return noise_info
        
        # å®Œå…¨æ— æ³•è¯†åˆ«çš„æƒ…å†µ
        logger.warning(f"âš ï¸ æ— æ³•ä»è·¯å¾„ '{noisy_path}' ä¸­è¯†åˆ«å™ªå£°ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        return {
            'root_type': 'unknown',
            'noise_type': 'unknown', 
            'db_value': 'unknown_db',
            'display_name': 'unknown-unknown-unknown_db'
        }

    def _setup_results_directory(self, experiment_name=None):
        """
        è®¾ç½®å¤šå±‚çº§ç»“æœç›®å½•ç»“æ„
        
        ç»“æ„ï¼š
        casia_cross_domain_results/
        â”œâ”€â”€ real_noise/
        â”‚   â”œâ”€â”€ babble/
        â”‚   â”‚   â”œâ”€â”€ 20db/
        â”‚   â”‚   â”‚   â””â”€â”€ fold_X/
        â”‚   â”‚   â””â”€â”€ 15db/
        â”‚   â”œâ”€â”€ factory1/
        â”‚   â”œâ”€â”€ hfchannel/
        â”‚   â””â”€â”€ volvo/
        â”œâ”€â”€ root1/
        â”‚   â”œâ”€â”€ babble/
        â”‚   â”‚   â”œâ”€â”€ 20db/
        â”‚   â”‚   â”‚   â””â”€â”€ fold_X/
        â”‚   â”‚   â””â”€â”€ 15db/
        â”‚   â””â”€â”€ white/
        â””â”€â”€ root2/
            â”œâ”€â”€ 20db/
            â”‚   â””â”€â”€ fold_X/
            â””â”€â”€ 15db/
        """
        base_dir = "casia_mutil-noise_cross_domain_results"
        
        if experiment_name:
            # å¦‚æœæä¾›äº†å®éªŒåç§°ï¼Œåœ¨æ ¹ç›®å½•ä¸‹åˆ›å»ºå®éªŒåç§°å­ç›®å½•
            safe_exp_name = re.sub(r'[\\/*?:"<>|]', "", experiment_name)
            base_dir = os.path.join(base_dir, safe_exp_name)
        
        # æ ¹æ®rootç±»å‹æ„å»ºè·¯å¾„
        root_type = self.noise_info['root_type']
        
        if root_type == 'real_noise':
            # real_noise: base/real_noise/noise_type/db/fold_X
            noise_type = self.noise_info['noise_type']
            db_value = self.noise_info['db_value']
            results_dir = os.path.join(base_dir, root_type, noise_type, db_value, f"fold_{self.fold+1}")
            
        elif root_type == 'root1':
            # root1: base/root1/noise_type/db/fold_X
            noise_type = self.noise_info['noise_type']
            db_value = self.noise_info['db_value']
            results_dir = os.path.join(base_dir, root_type, noise_type, db_value, f"fold_{self.fold+1}")
            
        elif root_type == 'root2':
            # root2: base/root2/db/fold_X
            db_value = self.noise_info['db_value']
            results_dir = os.path.join(base_dir, root_type, db_value, f"fold_{self.fold+1}")
            
        else:
            # unknown: base/unknown/fold_X
            results_dir = os.path.join(base_dir, "unknown", f"fold_{self.fold+1}")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        os.makedirs(results_dir, exist_ok=True)
        os.makedirs(os.path.join(results_dir, "models"), exist_ok=True)
        os.makedirs(os.path.join(results_dir, "plots"), exist_ok=True)
        os.makedirs(os.path.join(results_dir, "reports"), exist_ok=True)
        
        logger.info(f"ğŸ“ å¤šå±‚çº§ç»“æœç›®å½•å·²åˆ›å»º: {results_dir}")
        logger.info(f"ğŸ“‚ ç›®å½•ç»“æ„: {self.noise_info['display_name']} â†’ fold_{self.fold+1}")
        
        return results_dir
    
    def _setup_components(self):
        """è®¾ç½®æ‰€æœ‰è®­ç»ƒç»„ä»¶"""
        self._setup_dataloaders()
        self._setup_model()
        self._load_pretrained_weights()
        
        # [æ–°å¢] è¿è¡Œä¸€æ¬¡æ€§çš„é”šç‚¹æ ¡å‡†
        if cfg.ANCHOR_CALIBRATION_ENABLED:
            self._run_anchor_calibration()
        
        self._setup_training_components()
        self._setup_augmentation()
    
    def _setup_dataloaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        logger.info("ğŸ—‚ï¸ è®¾ç½®CASIAæ•°æ®åŠ è½½å™¨...")
        try:
            self.clean_train_loader, self.clean_val_loader, self.clean_test_loader, self.num_classes, self.class_names = \
                create_casia_clean_dataloaders(cfg.CLEAN_FEAT_PATH, cfg.BATCH_SIZE, fold=self.fold)
            
            self.noisy_student_loader, self.noisy_teacher_loader, self.noisy_val_loader, self.noisy_test_loader = \
                create_casia_noisy_dataloaders_with_speaker_isolation(
                    cfg.NOISY_FEAT_PATH, cfg.BATCH_SIZE, fold=self.fold
                )
            
            logger.info(f"âœ… CASIAæ•°æ®åŠ è½½å™¨è®¾ç½®å®Œæˆ (Fold {self.fold+1}/4)")
            logger.info(f"   ğŸ§¹ å¹²å‡€æ•°æ® - è®­ç»ƒ: {len(self.clean_train_loader)} æ‰¹æ¬¡, éªŒè¯: {len(self.clean_val_loader)} æ‰¹æ¬¡, æµ‹è¯•: {len(self.clean_test_loader)} æ‰¹æ¬¡")
            logger.info(f"   ğŸ”‡ å™ªå£°æ•°æ® - è®­ç»ƒ: {len(self.noisy_student_loader)} æ‰¹æ¬¡, éªŒè¯: {len(self.noisy_val_loader)} æ‰¹æ¬¡, æµ‹è¯•: {len(self.noisy_test_loader)} æ‰¹æ¬¡")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å™¨è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        logger.info("ğŸ—ï¸ è®¾ç½®CASIAè·¨åŸŸSSRLæ¨¡å‹...")
        self.model = SSRLModel(cfg=cfg).to(self.device)
        if cfg.PRINT_MODEL_INFO:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        logger.info("âœ… CASIAè·¨åŸŸSSRLæ¨¡å‹è®¾ç½®å®Œæˆ")

    def _load_pretrained_weights(self):
        """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        logger.info(f"ğŸ’¾ åŠ è½½CASIAé¢„è®­ç»ƒæƒé‡: {cfg.PRETRAINED_CASIA_WEIGHT}")
        if not os.path.exists(cfg.PRETRAINED_CASIA_WEIGHT):
            logger.error(f"âŒ é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨!")
            raise FileNotFoundError(f"é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {cfg.PRETRAINED_CASIA_WEIGHT}")
        
        try:
            self.model.load_complete_pretrained_weights(cfg.PRETRAINED_CASIA_WEIGHT)
            self.model._init_teacher_network()
            logger.info("âœ… æˆåŠŸå°†é¢„è®­ç»ƒæƒé‡åŠ è½½åˆ°å­¦ç”Ÿç½‘ç»œå¹¶åŒæ­¥åˆ°æ•™å¸ˆç½‘ç»œã€‚")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
            raise

    def _run_anchor_calibration(self):
        """[æ–°å¢] æ‰§è¡Œä¸€æ¬¡æ€§çš„é”šç‚¹æ ¡å‡† (è®ºæ–‡ 3.2 èŠ‚) [cite: 5, 12]"""
        logger.info("âš“ï¸ æ­£åœ¨æ‰§è¡Œä¸€æ¬¡æ€§é”šç‚¹æ ¡å‡† (Anchor Calibration)...")
        # ä¸´æ—¶æ•°æ®åŠ è½½å™¨ï¼Œéå†æ‰€æœ‰æ•°æ®
        clean_calib_loader, _, _, _, _ = create_casia_clean_dataloaders(cfg.CLEAN_FEAT_PATH, cfg.BATCH_SIZE * 2, fold=self.fold)
        _, _, noisy_calib_loader, _ = create_casia_noisy_dataloaders_with_speaker_isolation(cfg.NOISY_FEAT_PATH, cfg.BATCH_SIZE * 2, fold=self.fold)

        self.model.eval()
        scores_per_class = {'clean': [[] for _ in range(self.num_classes)], 'noisy': [[] for _ in range(self.num_classes)]}

        with torch.no_grad():
            # æ”¶é›†å¹²å‡€æ•°æ®åˆ†æ•°
            for batch in clean_calib_loader:
                feats, mask, labels = batch['net_input']['feats'].to(self.device), batch['net_input']['padding_mask'].to(self.device), batch['labels'].to(self.device)
                probs = F.softmax(self.model.predict(feats, mask), dim=1)
                # [cite_start]scores, _ = DACPManager.calculate_certainty_scores(probs) # [cite: 17]
                scores, _ = DACPManager.calculate_certainty_scores(probs)
                for i, label in enumerate(labels):
                    scores_per_class['clean'][label.item()].append(scores[i].item())
            
            # æ”¶é›†å™ªå£°æ•°æ®åˆ†æ•°
            for batch in noisy_calib_loader:
                feats, mask, labels = batch['net_input']['feats'].to(self.device), batch['net_input']['padding_mask'].to(self.device), batch['labels'].to(self.device)
                probs = F.softmax(self.model.predict(feats, mask), dim=1)
                # [cite_start]scores, _ = DACPManager.calculate_certainty_scores(probs) # [cite: 17]
                scores, _ = DACPManager.calculate_certainty_scores(probs)
                for i, label in enumerate(labels):
                    scores_per_class['noisy'][label.item()].append(scores[i].item())
        
        # è®¡ç®—åç§»å’Œæœ€ç»ˆé”šç‚¹ (å…¬å¼ 1-6)
        avg_scores_clean = torch.tensor([np.mean(s) if s else 0 for s in scores_per_class['clean']], device=self.device)
        avg_scores_noisy = torch.tensor([np.mean(s) if s else 0 for s in scores_per_class['noisy']], device=self.device)
        
        # [cite_start]shift_ratio = avg_scores_noisy / (avg_scores_clean + 1e-8) # [cite: 21]
        shift_ratio = avg_scores_noisy / (avg_scores_clean + 1e-8)
        
        # [cite_start]mu_clean = avg_scores_clean # [cite: 25]
        mu_clean = avg_scores_clean
        # [cite_start]sigma_clean = torch.tensor([np.std(s) if s else 0 for s in scores_per_class['clean']], device=self.device) # [cite: 26]
        sigma_clean = torch.tensor([np.std(s) if s else 0 for s in scores_per_class['clean']], device=self.device)
        
        # [cite_start]base_anchor = torch.clamp(mu_clean - cfg.ANCHOR_STD_K * sigma_clean, min=0) # [cite: 28]
        base_anchor = torch.clamp(mu_clean - cfg.ANCHOR_STD_K * sigma_clean, min=0)
        
        # [cite_start]self.calibrated_anchors = base_anchor * shift_ratio # [cite: 31]
        self.calibrated_anchors = base_anchor * shift_ratio
        
        logger.info("âœ… é”šç‚¹æ ¡å‡†å®Œæˆ!")
        for c, anchor in enumerate(self.calibrated_anchors):
            logger.info(f"   - ç±»åˆ« '{self.class_names[c]}': æ ¡å‡†é”šç‚¹ Ï„_calibrated_anchor = {anchor:.4f}")
        
        self.model.train()

    def _setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        logger.info("âš™ï¸ è®¾ç½®è·¨åŸŸè®­ç»ƒç»„ä»¶ (DACP+ECDA)...")
        self.optimizer = optim.Adam(self.model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=cfg.EPOCHS) if cfg.LEARNING_RATE_SCHEDULER == "cosine" else None
        self.ce_criterion = nn.CrossEntropyLoss(label_smoothing=cfg.LABEL_SMOOTHING_FACTOR if cfg.USE_LABEL_SMOOTHING else 0.0)
        self.kl_criterion = nn.KLDivLoss(reduction='none') # ä¿®æ”¹ä¸º 'none' ä»¥ä¾¿æ‰‹åŠ¨åŠ æƒ
        # self.scl_criterion = SupervisedContrastiveLoss(temperature=0.1) # [åˆ é™¤]
        
        self.dacp_manager = DACPManager(self.num_classes, cfg.EPOCHS, self.device)
        self.ecda_criterion = ECDALoss()
        
        logger.info(f"âœ… è·¨åŸŸè®­ç»ƒç»„ä»¶è®¾ç½®å®Œæˆ (å­¦ä¹ ç‡: {cfg.LEARNING_RATE})")

    def _setup_augmentation(self):
        """è®¾ç½®æ•°æ®å¢å¼ºå™¨"""
        self.augmenter = DataAugmentation()
        logger.info("âœ… æ•°æ®å¢å¼ºå™¨è®¾ç½®å®Œæˆ")

    def is_warmup_phase(self, epoch):
        return epoch < self.WARMUP_EPOCHS

    def update_loss_weights(self, epoch):
        if self.is_warmup_phase(epoch):
            self.weight_scl, self.weight_ecda, self.current_consistency_weight = 0.0, 0.0, 0.0
            return
        
        # é˜¶æ®µäºŒ: ç¨³å®šæ•™å¸ˆç½‘ç»œ (ä»…ä¸€è‡´æ€§æŸå¤±)
        if cfg.PROGRESSIVE_TRAINING:
            # æ­¤å¤„progressç”¨äºä¸€è‡´æ€§æŸå¤±çš„æƒé‡ ramp-up
            progress = min(1.0, (epoch - self.WARMUP_EPOCHS) / cfg.WEIGHT_RAMP_EPOCHS)
            self.current_consistency_weight = self.initial_consistency_weight + (self.final_consistency_weight - self.initial_consistency_weight) * progress
        else:
            self.current_consistency_weight = cfg.WEIGHT_CONSISTENCY
        
        # SCL å§‹ç»ˆç¦ç”¨
        self.weight_scl = 0.0
        
        # é˜¶æ®µä¸‰: å¯åŠ¨ ECDA (åœ¨ç¨³å®šé˜¶æ®µä¹‹å)
        if epoch >= cfg.ECDA_START_EPOCH:
            # æ­¤å¤„progressç”¨äºECDAæŸå¤±çš„æƒé‡ ramp-up
            ecda_progress = min(1.0, (epoch - cfg.ECDA_START_EPOCH) / cfg.WEIGHT_RAMP_EPOCHS)
            self.weight_ecda = cfg.WEIGHT_ECDA * ecda_progress
        else:
            self.weight_ecda = 0.0

    def train_step(self, clean_batch, noisy_batch, epoch):
        # [cite_start]1. å¹²å‡€æ•°æ®ç›‘ç£éƒ¨åˆ† (L_CE) [cite: 122]
        clean_feats, clean_padding_mask, clean_labels = clean_batch['net_input']['feats'].to(self.device), clean_batch['net_input']['padding_mask'].to(self.device), clean_batch['labels'].to(self.device)
        student_clean_encoded = self.model.student_encoder(clean_feats, clean_padding_mask)
        supervised_ce_loss = self.ce_criterion(self.model.student_classifier(student_clean_encoded), clean_labels)
        
        # SCLæŸå¤± (å¦‚æœå¯ç”¨)
        scl_loss = torch.tensor(0.0, device=self.device) # [ä¿®æ”¹] ç›´æ¥è®¾ç½®ä¸º0
        
        if self.is_warmup_phase(epoch):
            total_loss = supervised_ce_loss # [ä¿®æ”¹] ç§»é™¤SCL
            return {'total_loss': total_loss, 'supervised_ce_loss': supervised_ce_loss, 'scl_loss': scl_loss, 'consistency_loss': torch.tensor(0.0, device=self.device), 'ecda_loss': torch.tensor(0.0, device=self.device)}
        
        # [cite_start]2. å™ªå£°æ•°æ®è’¸é¦éƒ¨åˆ† (L_KD, L_ECDA) [cite: 6]
        noisy_feats, noisy_padding_mask = noisy_batch['net_input']['feats'].to(self.device), noisy_batch['net_input']['padding_mask'].to(self.device)
        noisy_weak, noisy_strong = self.augmenter.weak_augment(noisy_feats), self.augmenter.strong_augment(noisy_feats)
        
        with torch.no_grad():
            teacher_encoded = self.model.teacher_encoder(noisy_weak, noisy_padding_mask)
            teacher_probs = F.softmax(self.model.teacher_classifier(teacher_encoded), dim=1)
        
        # [cite_start][ä¿®æ”¹] ä½¿ç”¨ DACP æ›¿ä»£æ—§çš„ç½®ä¿¡åº¦ç­›é€‰ [cite: 7]
        if cfg.USE_DACP:
            mask, certainty_scores, class_weights_wce = self.dacp_manager.calculate_mask(
                teacher_probs, epoch, self.calibrated_anchors
            )
        else:
            # [æ–°å¢] ä½¿ç”¨å›ºå®šé˜ˆå€¼è¿›è¡Œç­›é€‰ (æ¶ˆèå®éªŒ)
            certainty_scores, pseudo_labels = torch.max(teacher_probs, dim=1)
            mask = (certainty_scores >= cfg.FIXED_CONFIDENCE_THRESHOLD).float()
            class_weights_wce = torch.ones_like(mask) # å›ºå®šé˜ˆå€¼æ—¶ä¸ä½¿ç”¨WCE

        high_confidence_count = mask.sum().item()
        
        student_strong_encoded = self.model.student_encoder(noisy_strong, noisy_padding_mask)
        student_log_probs = F.log_softmax(self.model.student_classifier(student_strong_encoded), dim=1)
        
        consistency_loss, ecda_loss = torch.tensor(0.0, device=self.device), torch.tensor(0.0, device=self.device)

        if high_confidence_count > 1: # ECDA éœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬
            # [cite_start]ä¸€è‡´æ€§æŸå¤± (L_KD) [cite: 124]
            kl_div = self.kl_criterion(student_log_probs, teacher_probs)
            kl_div_per_sample = kl_div.sum(dim=1)
            # åŠ æƒæ±‚å’Œï¼Œç„¶åé™¤ä»¥æ€»æ ·æœ¬æ•°ï¼Œç­‰æ•ˆäºåªå¯¹æ©ç éƒ¨åˆ†æ±‚å‡å€¼
            consistency_loss = (kl_div_per_sample * mask).sum() / (mask.sum() + 1e-8)
            
            # [cite_start]ECDA æŸå¤± [cite: 126]
            if cfg.USE_ECDA and self.weight_ecda > 0:
                _, noisy_pseudo_labels = torch.max(teacher_probs, dim=1)
                ecda_loss = self.ecda_criterion(
                    clean_feats=student_clean_encoded,
                    noisy_feats=student_strong_encoded,
                    clean_labels=clean_labels,
                    noisy_labels=noisy_pseudo_labels,
                    noisy_mask=mask,
                    noisy_scores=certainty_scores,
                    class_weights_wce=class_weights_wce
                )

        # [cite_start]3. è®¡ç®—æ€»æŸå¤± (å…¬å¼ 32) [cite: 129]
        total_loss = (
            supervised_ce_loss + 
            # self.weight_scl * scl_loss + # [åˆ é™¤]
            self.current_consistency_weight * consistency_loss +
            self.weight_ecda * ecda_loss
        )
        
        return {
            'total_loss': total_loss, 'supervised_ce_loss': supervised_ce_loss, 
            'scl_loss': scl_loss, 'consistency_loss': consistency_loss, 'ecda_loss': ecda_loss
        }

    def train_epoch(self, epoch):
        self.model.train()
        self.update_loss_weights(epoch)
        total_losses = defaultdict(float)
        num_batches = 0
        
        clean_iter, noisy_student_iter = iter(self.clean_train_loader), iter(self.noisy_student_loader)
        max_batches = min(len(self.clean_train_loader), len(self.noisy_student_loader))

        for batch_idx in range(max_batches):
            clean_batch, noisy_batch = next(clean_iter), next(noisy_student_iter)
            self.optimizer.zero_grad()
            losses = self.train_step(clean_batch, noisy_batch, epoch)
            losses['total_loss'].backward()
            if cfg.GRADIENT_CLIPPING:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), cfg.MAX_GRAD_NORM)
            self.optimizer.step()
            
            # [cite_start]EMA æ›´æ–°æ•™å¸ˆç½‘ç»œ [cite: 8]
            if not self.is_warmup_phase(epoch):
                self.model.update_teacher_ema()
                
            for key, value in losses.items():
                if isinstance(value, torch.Tensor): total_losses[key] += value.item()
            num_batches += 1
        
        # [æ–°å¢] epoch ç»“æŸåæ›´æ–° DACP çš„ç±»åˆ«è´¨é‡åˆ†æ•°
        if not self.is_warmup_phase(epoch):
            self.dacp_manager.update_class_quality_scores_epoch(self.dacp_manager.batch_scores_per_class)
        
        if self.scheduler: self.scheduler.step()
        return {key: value / num_batches for key, value in total_losses.items()}

    def validate(self, data_loader, domain_name="unknown"):
        """éªŒè¯å‡½æ•° - è®¡ç®—è¯¦ç»†è¯„ä¼°æŒ‡æ ‡"""
        self.model.eval()
        all_preds, all_labels = [], []
        with torch.no_grad():
            for batch in data_loader:
                feats, mask, labels = batch['net_input']['feats'].to(self.device), batch['net_input']['padding_mask'].to(self.device), batch['labels'].to(self.device)
                outputs = self.model.predict(feats, mask, use_teacher=False)
                _, predicted = torch.max(outputs, 1)
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        cm = confusion_matrix(all_labels, all_preds, labels=range(self.num_classes))
        
        (prec, rec, f1, sup) = precision_recall_fscore_support(all_labels, all_preds, average=None, zero_division=0, labels=range(self.num_classes))

        return {
            'accuracy': accuracy_score(all_labels, all_preds) * 100,
            'weighted_accuracy': balanced_accuracy_score(all_labels, all_preds) * 100,
            'f1_weighted': f1_score(all_labels, all_preds, average='weighted', zero_division=0) * 100,
            'f1_macro': f1_score(all_labels, all_preds, average='macro', zero_division=0) * 100,
            'precision_per_class': prec.tolist(), 'recall_per_class': rec.tolist(),
            'f1_per_class': f1.tolist(), 'support_per_class': sup.tolist(),
            'confusion_matrix': cm
        }

    def check_early_stopping(self, noisy_results, is_best):
        """æ—©åœæ£€æŸ¥ - æ ¸å¿ƒç›‘æ§å™ªå£°åŸŸåŠ æƒå‡†ç¡®ç‡"""
        if not cfg.EARLY_STOPPING: return False
        if is_best:
            self.patience_counter = 0
            logger.info(f"ğŸ¯ å‘ç°æ›´ä½³æ¨¡å‹! å™ªå£°åŸŸåŠ æƒå‡†ç¡®ç‡: {noisy_results['weighted_accuracy']:.2f}% - æ—©åœè®¡æ•°å™¨é‡ç½®")
            return False
        else:
            self.patience_counter += 1
            logger.info(f"â³ æ—©åœè®¡æ•°: {self.patience_counter}/{cfg.PATIENCE} (å½“å‰å™ªå£°åŸŸåŠ æƒå‡†ç¡®ç‡: {noisy_results['weighted_accuracy']:.2f}%)")
            if self.patience_counter >= cfg.PATIENCE:
                logger.info(f"ğŸ›‘ æ—©åœè§¦å‘!")
                return True
        return False

    def save_checkpoint(self, epoch, clean_results, noisy_results, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {'epoch': epoch, 'model_state_dict': self.model.state_dict(), 'optimizer_state_dict': self.optimizer.state_dict(), 'clean_results': clean_results, 'noisy_results': noisy_results}
        models_dir = os.path.join(self.results_dir, "models")
        
        if is_best:
            self.best_results.update({'epoch': epoch, 'clean_results': clean_results, 'noisy_results': noisy_results})
            best_path = os.path.join(models_dir, f'casia_cross_domain_best.pth')
            torch.save(checkpoint, best_path)
            self._save_confusion_matrices(clean_results, noisy_results, epoch, is_best_result=True)
            self._save_detailed_results(clean_results, noisy_results, epoch, is_best_result=True)
            logger.info(f"ğŸ’¾ Best model saved: {best_path}")

    def _save_confusion_matrices(self, clean_results, noisy_results, epoch, is_best_result=False):
        """ç”Ÿæˆå¹¶ä¿å­˜æ··æ·†çŸ©é˜µå›¾"""
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))
        sns.heatmap(clean_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues', xticklabels=self.class_names, yticklabels=self.class_names, ax=ax1)
        ax1.set_title(f'Clean Domain (Epoch {epoch+1})\nAcc: {clean_results["accuracy"]:.2f}%, W-Acc: {clean_results["weighted_accuracy"]:.2f}%')
        sns.heatmap(noisy_results['confusion_matrix'], annot=True, fmt='d', cmap='Oranges', xticklabels=self.class_names, yticklabels=self.class_names, ax=ax2)
        ax2.set_title(f'Noisy Domain ({self.noise_info["display_name"]}) (Epoch {epoch+1})\nAcc: {noisy_results["accuracy"]:.2f}%, W-Acc: {noisy_results["weighted_accuracy"]:.2f}%')
        if is_best_result: fig.suptitle('ğŸ† BEST RESULTS ğŸ†', fontsize=16, weight='bold')
        plots_dir = os.path.join(self.results_dir, "plots")
        filename = f'BEST_confusion_matrices_epoch_{epoch+1}.png' if is_best_result else f'confusion_matrices_epoch_{epoch+1}.png'
        plt.savefig(os.path.join(plots_dir, filename), dpi=300)
        plt.close()

    def _save_detailed_results(self, clean_results, noisy_results, epoch, is_best_result=False):
        """ä¿å­˜è¯¦ç»†JSONç»“æœæŠ¥å‘Š"""
        def _calculate_per_class_accuracy(cm):
            return [(cm[i, i] / cm[i, :].sum()) if cm[i, :].sum() > 0 else 0.0 for i in range(len(cm))]
        
        results_summary = {
            'info': {'noise_config': self.noise_info, 'fold': self.fold + 1, 'epoch': epoch + 1, 'is_best': is_best_result},
            'summary': {
                'clean': {'acc': f"{clean_results['accuracy']:.2f}%", 'w_acc': f"{clean_results['weighted_accuracy']:.2f}%", 'w_f1': f"{clean_results['f1_weighted']:.2f}%"},
                'noisy': {'acc': f"{noisy_results['accuracy']:.2f}%", 'w_acc': f"{noisy_results['weighted_accuracy']:.2f}%", 'w_f1': f"{noisy_results['f1_weighted']:.2f}%"}
            },
            'details': {
                'class_names': self.class_names,
                'clean': {'precision': clean_results['precision_per_class'], 'recall': clean_results['recall_per_class'], 'f1': clean_results['f1_per_class'], 'support': clean_results['support_per_class'], 'accuracy': _calculate_per_class_accuracy(clean_results['confusion_matrix'])},
                'noisy': {'precision': noisy_results['precision_per_class'], 'recall': noisy_results['recall_per_class'], 'f1': noisy_results['f1_per_class'], 'support': noisy_results['support_per_class'], 'accuracy': _calculate_per_class_accuracy(noisy_results['confusion_matrix'])}
            }
        }
        reports_dir = os.path.join(self.results_dir, "reports")
        filename = f'BEST_detailed_results_epoch_{epoch+1}.json' if is_best_result else f'detailed_results_epoch_{epoch+1}.json'
        with open(os.path.join(reports_dir, filename), 'w', encoding='utf-8') as f:
            json.dump(results_summary, f, indent=4, ensure_ascii=False)

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info(f"ğŸš€ å¼€å§‹CASIAè·¨åŸŸè®­ç»ƒ (Fold {self.fold+1}/4, å™ªå£°: {self.noise_info['display_name']})...")
        for epoch in range(cfg.EPOCHS):
            avg_losses = self.train_epoch(epoch)
            for key, value in avg_losses.items(): self.training_history[key].append(value)
            
            should_validate = (epoch + 1) % cfg.VALIDATION_INTERVAL == 0 or not self.is_warmup_phase(epoch)
            if should_validate:
                clean_results = self.validate(self.clean_val_loader, "Clean")
                noisy_results = self.validate(self.noisy_val_loader, "Noisy")
                
                is_best = noisy_results['weighted_accuracy'] > self.best_noisy_weighted_acc + cfg.MIN_DELTA
                if is_best:
                    self.best_noisy_acc, self.best_clean_acc = noisy_results['accuracy'], clean_results['accuracy']
                    self.best_noisy_weighted_f1, self.best_clean_weighted_f1 = noisy_results['f1_weighted'], clean_results['f1_weighted']
                    self.best_noisy_weighted_acc, self.best_clean_weighted_acc = noisy_results['weighted_accuracy'], clean_results['weighted_accuracy']

                self.save_checkpoint(epoch, clean_results, noisy_results, is_best)
                
                logger.info(f"Epoch {epoch+1}/{cfg.EPOCHS} | Losses: Total={avg_losses['total_loss']:.4f} CE={avg_losses['supervised_ce_loss']:.4f} KD={avg_losses['consistency_loss']:.4f} ECDA={avg_losses['ecda_loss']:.4f} | Noisy WA: {noisy_results['weighted_accuracy']:.2f}% {'ğŸ”¥' if is_best else ''}")
                
                if self.check_early_stopping(noisy_results, is_best): break
        
        # [æ–°å¢] åŠ è½½æœ€ä½³æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
        logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°...")
        self._evaluate_on_test_set()
        
        logger.info(f"ğŸ‰ CASIAè·¨åŸŸè®­ç»ƒå®Œæˆ (Fold {self.fold+1})!")
        return {'best_noisy_weighted_acc': self.best_noisy_weighted_acc, 'results_dir': self.results_dir}

    def _evaluate_on_test_set(self):
        """åŠ è½½æœ€ä½³æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°"""
        best_model_path = os.path.join(self.results_dir, "models", "casia_cross_domain_best.pth")
        if not os.path.exists(best_model_path):
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_model_path}ã€‚è·³è¿‡æµ‹è¯•é›†è¯„ä¼°ã€‚")
            return

        logger.info(f"ğŸ’¾ åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}")
        checkpoint = torch.load(best_model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        # è¯„ä¼°å¹²å‡€åŸŸ
        logger.info("ğŸ§¹ è¯„ä¼°å¹²å‡€åŸŸæµ‹è¯•é›†...")
        clean_test_results = self.validate(self.clean_test_loader, "Clean_Test")
        logger.info(f"   ğŸ“Š Clean Domain Test - Acc: {clean_test_results['accuracy']:.2f}%, W-Acc: {clean_test_results['weighted_accuracy']:.2f}%, W-F1: {clean_test_results['f1_weighted']:.2f}%")

        # è¯„ä¼°å™ªå£°åŸŸ
        logger.info("ğŸ”‡ è¯„ä¼°å™ªå£°åŸŸæµ‹è¯•é›†...")
        noisy_test_results = self.validate(self.noisy_test_loader, "Noisy_Test")
        logger.info(f"   ğŸ“Š Noisy Domain Test - Acc: {noisy_test_results['accuracy']:.2f}%, W-Acc: {noisy_test_results['weighted_accuracy']:.2f}%, W-F1: {noisy_test_results['f1_weighted']:.2f}%")

        # ä¿å­˜æœ€ç»ˆæµ‹è¯•ç»“æœ
        self._save_confusion_matrices(clean_test_results, noisy_test_results, cfg.EPOCHS-1, is_best_result=False)
        self._save_detailed_results(clean_test_results, noisy_test_results, cfg.EPOCHS-1, is_best_result=False)
        
        # ç”Ÿæˆå¹¶ä¿å­˜æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š
        final_test_summary = {
            'info': {
                'noise_config': self.noise_info,
                'fold': self.fold + 1,
                'evaluation_type': 'Final Test Set Evaluation',
                'timestamp': datetime.now().isoformat()
            },
            'final_test_results': {
                'clean_domain': {
                    'accuracy': f"{clean_test_results['accuracy']:.2f}%",
                    'weighted_accuracy': f"{clean_test_results['weighted_accuracy']:.2f}%",
                    'weighted_f1': f"{clean_test_results['f1_weighted']:.2f}%"
                },
                'noisy_domain': {
                    'accuracy': f"{noisy_test_results['accuracy']:.2f}%",
                    'weighted_accuracy': f"{noisy_test_results['weighted_accuracy']:.2f}%",
                    'weighted_f1': f"{noisy_test_results['f1_weighted']:.2f}%"
                }
            }
        }
        
        test_report_path = os.path.join(self.results_dir, "reports", "FINAL_test_set_results.json")
        with open(test_report_path, 'w', encoding='utf-8') as f:
            json.dump(final_test_summary, f, indent=4, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ æœ€ç»ˆæµ‹è¯•é›†ç»“æœå·²ä¿å­˜è‡³: {test_report_path}")
        logger.info("âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆ!")
        
        return clean_test_results, noisy_test_results

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨CASIAè·¨åŸŸè®­ç»ƒå™¨ (DACP+ECDA å¢å¼ºç‰ˆ)...")
    fold = 3
    logger.info(f"ğŸ“ CASIAè·¨åŸŸè®­ç»ƒ - ä½¿ç”¨Fold {fold+1}é¢„è®­ç»ƒæƒé‡")
    
    trainer = FixedCASIACrossDomainTrainer(fold=fold)
    results = trainer.train()
    
    logger.info(f"âœ… CASIAè·¨åŸŸè®­ç»ƒå®Œæˆ!")
    logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœç»Ÿè®¡ (æ ¸å¿ƒæŒ‡æ ‡: å™ªå£°åŸŸåŠ æƒå‡†ç¡®ç‡):")
    logger.info(f"   ğŸ¯ æœ€ä½³æ€§èƒ½: {results['best_noisy_weighted_acc']:.2f}%")
    logger.info(f"   ğŸ“‹ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {results['results_dir']}")

if __name__ == "__main__":
    main()