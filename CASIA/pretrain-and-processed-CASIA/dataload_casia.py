import os
import logging
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
# ç§»é™¤'import config'ï¼Œè§£é™¤å¯¹ç‰¹å®šæ–‡ä»¶çš„ä¾èµ–
# import config as cfg

# ä»æ‚¨ç°æœ‰çš„ dataload_clean.py ä¸­å¤ç”¨åŸºç¡€çš„Datasetç±»
from dataload_clean import CleanEmotionDatasetFromArrays

logger = logging.getLogger(__name__)

# load_casia_data å’Œ create_casia_speaker_isolated_loaders å‡½æ•°ä¿æŒä¸å˜...
def load_casia_data(feature_path, label_dict):
    """
    åŠ è½½å®Œæ•´çš„CASIAæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç‰¹å¾ã€æ ‡ç­¾å’Œè¯´è¯äººIDã€‚
    """
    # åŠ è½½ç‰¹å¾
    feats = np.load(f"{feature_path}.npy")
    
    # åŠ è½½æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦
    with open(f"{feature_path}.lengths", "r") as f:
        sizes = [int(line.strip()) for line in f]
    
    # åŠ è½½æ ‡ç­¾å¹¶è½¬æ¢ä¸ºæ•°å­—
    with open(f"{feature_path}.lbl", "r", encoding="utf-8") as f:
        labels = [label_dict[line.strip()] for line in f]
        
    # åŠ è½½è¯´è¯äººID
    with open(f"{feature_path}.spk", "r", encoding="utf-8") as f:
        speakers = [line.strip() for line in f]
        
    # è®¡ç®—åç§»é‡
    offsets = np.concatenate([[0], np.cumsum(sizes)[:-1]])
    
    dataset = {
        "feats": feats,
        "sizes": np.array(sizes),
        "offsets": np.array(offsets),
        "labels": np.array(labels),
        "speakers": np.array(speakers),
        "num": len(labels),
    }
    
    print(f"âœ… CASIAæ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {dataset['num']} ä¸ªæ ·æœ¬ã€‚")
    return dataset


def create_casia_speaker_isolated_loaders(dataset, fold, batch_size):
    """
    ã€æ ¸å¿ƒå‡½æ•°ã€‘ä¸ºCASIAæ•°æ®é›†åˆ›å»ºä¸¥æ ¼æŒ‰è¯´è¯äººéš”ç¦»çš„æ•°æ®åŠ è½½å™¨ã€‚
    è¿™æ˜¯ä¸€ä¸ª4æŠ˜äº¤å‰éªŒè¯çš„å®ç°ã€‚
    - è®­ç»ƒé›†: 2ä¸ªè¯´è¯äºº
    - éªŒè¯é›†: 1ä¸ªè¯´è¯äºº
    - æµ‹è¯•é›†: 1ä¸ªè¯´è¯äºº (foldå¯¹åº”çš„è¯´è¯äºº)
    """
    all_speakers = np.unique(dataset['speakers'])
    if len(all_speakers) != 4:
        raise ValueError(f"é¢„æœŸCASIAæœ‰4ä¸ªè¯´è¯äºº, ä½†åœ¨.spkæ–‡ä»¶ä¸­æ‰¾åˆ°äº† {len(all_speakers)} ä¸ªã€‚")

    # ç¡®å®šå½“å‰æŠ˜çš„è¯´è¯äººåˆ†é…
    test_speaker = all_speakers[fold]
    val_speaker = all_speakers[(fold + 1) % 4] # ä½¿ç”¨ä¸‹ä¸€ä¸ªè¯´è¯äººåšéªŒè¯
    train_speakers = [spk for spk in all_speakers if spk not in [test_speaker, val_speaker]]

    print("ğŸ“Š [CASIA Data] Speaker Isolation Strategy (4-Fold):")
    print(f"  -  Fold: {fold + 1}/4")
    print(f"  - ğŸ‹ï¸  Training Speakers: {train_speakers}")
    print(f"  - ğŸ§ Validation Speaker: {val_speaker}")
    print(f"  - ğŸ§ª Test Speaker: {test_speaker}")

    # æ ¹æ®è¯´è¯äººIDè·å–æ ·æœ¬ç´¢å¼•
    train_indices = np.where(np.isin(dataset['speakers'], train_speakers))[0]
    val_indices = np.where(dataset['speakers'] == val_speaker)[0]
    test_indices = np.where(dataset['speakers'] == test_speaker)[0]

    np.random.shuffle(train_indices) # æ‰“ä¹±è®­ç»ƒé›†

    def create_subset(indices):
        """è¾…åŠ©å‡½æ•°ï¼šä»ç´¢å¼•åˆ—è¡¨åˆ›å»ºæ•°æ®é›†å­é›†"""
        sub_labels = dataset['labels'][indices]
        sub_sizes = dataset['sizes'][indices]
        sub_offsets = dataset['offsets'][indices]
        
        # ä¸ºäº†åˆ›å»ºæ–°çš„è¿ç»­ç‰¹å¾æ•°ç»„ï¼Œæˆ‘ä»¬éœ€è¦ä»åŸå§‹featsä¸­æå–ç‰‡æ®µ
        # æ³¨æ„ï¼šè¿™éƒ¨åˆ†ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ï¼Œä½†èƒ½ç¡®ä¿æ•°æ®éš”ç¦»å’ŒDatasetç±»çš„å…¼å®¹æ€§
        new_feats_list = [dataset['feats'][offset:offset+size] for offset, size in zip(sub_offsets, sub_sizes)]
        
        new_feats = np.concatenate(new_feats_list, axis=0)
        new_sizes = np.array([feat.shape[0] for feat in new_feats_list])
        new_offsets = np.concatenate([np.array([0]), np.cumsum(new_sizes)[:-1]], dtype=np.int64)

        return CleanEmotionDatasetFromArrays(new_feats, new_sizes, new_offsets, sub_labels, class_names=['angry', 'happy', 'neutral', 'sad'])

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = create_subset(train_indices)
    val_dataset = create_subset(val_indices)
    test_dataset = create_subset(test_indices)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_dataset.collator, num_workers=0, pin_memory=False, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=val_dataset.collator, num_workers=0, pin_memory=False, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collator, num_workers=0, pin_memory=False, shuffle=False)
                            
    return train_loader, val_loader, test_loader

# --- é‡ç‚¹ä¿®æ”¹åŒºåŸŸ ---
def create_casia_dataloaders(config, fold=0):
    """
    ã€å¯¹å¤–æ¥å£ã€‘ä¸ºCASIAæ•°æ®é›†åˆ›å»ºåŠ è½½å™¨ã€‚
    è¿™ä¸ªå‡½æ•°ç°åœ¨ç›´æ¥æ¥æ”¶ä¸€ä¸ªconfigå¯¹è±¡ã€‚
    """
    batch_size = config.BATCH_SIZE
    data_path = config.FEAT_PATH
    label_dict = config.LABEL_DICT
    
    print("ğŸ“‚ æ­£åœ¨åŠ è½½CASIAæ•°æ® (ä¸¥æ ¼è¯´è¯äººéš”ç¦»æ¨¡å¼)...")
    
    dataset = load_casia_data(data_path, label_dict)
    
    train_loader, val_loader, test_loader = create_casia_speaker_isolated_loaders(
        dataset, fold, batch_size
    )
    
    num_classes = len(label_dict)
    idx2label = {v: k for k, v in label_dict.items()}
    
    print(f"âœ… CASIAæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ (Fold {fold+1}):")
    print(f"   - è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)} (æ¥è‡ª {len(train_loader.dataset)} ä¸ªæ ·æœ¬)")
    print(f"   - éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)} (æ¥è‡ª {len(val_loader.dataset)} ä¸ªæ ·æœ¬)")
    print(f"   - æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)} (æ¥è‡ª {len(test_loader.dataset)} ä¸ªæ ·æœ¬)")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    return train_loader, val_loader, test_loader, num_classes, idx2label 